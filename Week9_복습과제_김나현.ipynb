{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **GPT-1 pytorch로 구현하기**\n",
        "- 링크 : [참고 블로그](https://m.blog.naver.com/PostView.naver?blogId=rbdus0715&logNo=223393183483&proxyReferer=https:%2F%2Fwww.google.com%2F&trackingCode=external)\n",
        "- 깃허브 : [주소](https://github.com/lyeoni/gpt-pytorch)\n",
        "- 트랜스포머 디코더 기반 모델로 기존 트랜스포머 구현 거의 동일하게 따라감."
      ],
      "metadata": {
        "id": "HY5NCPUrWA7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies"
      ],
      "metadata": {
        "id": "bhmFVObWuPxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install prenlp\n",
        "! git clone https://github.com/LiyuanLucasLiu/RAdam\n",
        "! python RAdam/setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMMJdKomuJIX",
        "outputId": "6e5e6e90-9b7d-44b3-ddc7-552b513c172f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prenlp\n",
            "  Downloading prenlp-0.0.13-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting nltk==3.2.5 (from prenlp)\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konlpy (from prenlp)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from prenlp) (0.2.0)\n",
            "Collecting ijson (from prenlp)\n",
            "  Downloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting py7zr==0.5b5 (from prenlp)\n",
            "  Downloading py7zr-0.5b5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from nltk==3.2.5->prenlp) (1.17.0)\n",
            "Collecting texttable (from py7zr==0.5b5->prenlp)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->prenlp)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy->prenlp) (24.2)\n",
            "Downloading prenlp-0.0.13-py3-none-any.whl (30 kB)\n",
            "Downloading py7zr-0.5b5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.2/119.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392144 sha256=447821fcbb9a3ae382ae961c1b1e0604e5d4d52307ea7e4ec612ea005ca4ec6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/69/e3/8b11e6490c8f20fcab5f6a3321d60fcc0b26ed6f7745ad95b4\n",
            "Successfully built nltk\n",
            "Installing collected packages: texttable, ijson, py7zr, nltk, JPype1, konlpy, prenlp\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JPype1-1.5.2 ijson-3.3.0 konlpy-0.6.0 nltk-3.2.5 prenlp-0.0.13 py7zr-0.5b5 texttable-1.7.0\n",
            "Cloning into 'RAdam'...\n",
            "remote: Enumerating objects: 320, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 320 (delta 1), reused 2 (delta 0), pack-reused 313 (from 1)\u001b[K\n",
            "Receiving objects: 100% (320/320), 955.36 KiB | 11.51 MiB/s, done.\n",
            "Resolving deltas: 100% (147/147), done.\n",
            "running install\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating RAdam.egg-info\n",
            "writing RAdam.egg-info/PKG-INFO\n",
            "writing dependency_links to RAdam.egg-info/dependency_links.txt\n",
            "writing requirements to RAdam.egg-info/requires.txt\n",
            "writing top-level names to RAdam.egg-info/top_level.txt\n",
            "writing manifest file 'RAdam.egg-info/SOURCES.txt'\n",
            "reading manifest file 'RAdam.egg-info/SOURCES.txt'\n",
            "writing manifest file 'RAdam.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying RAdam.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying RAdam.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying RAdam.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying RAdam.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying RAdam.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/RAdam-0.0.1-py3.11.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing RAdam-0.0.1-py3.11.egg\n",
            "Copying RAdam-0.0.1-py3.11.egg to /usr/local/lib/python3.11/dist-packages\n",
            "Adding RAdam 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/RAdam-0.0.1-py3.11.egg\n",
            "Processing dependencies for RAdam==0.0.1\n",
            "Searching for nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-nvjitlink-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=06b3b9b25bf3f8af351d664978ca26a16d2c5127dbd53c0497e28d1fb9611d57\n",
            "Best match: nvidia-nvjitlink-cu12 12.4.127\n",
            "Processing nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-nvjitlink-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './RAdam-0.0.1-py3.11.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cusparse-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl#sha256=ea4f11a2904e2a8dc4b1833cc1b5181cde564edd0d5cd33e3c168eff2d1863f1\n",
            "Best match: nvidia-cusparse-cu12 12.3.1.170\n",
            "Processing nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cusparse-cu12 12.3.1.170 to easy-install.pth file\n",
            "detected new path './nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cusolver-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl#sha256=19e33fa442bcfd085b3086c4ebf7e8debc07cfe01e11513cc6d332fd918ac260\n",
            "Best match: nvidia-cusolver-cu12 11.6.1.9\n",
            "Processing nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cusolver-cu12 11.6.1.9 to easy-install.pth file\n",
            "detected new path './nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-curand-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl#sha256=a88f583d4e0bb643c49743469964103aa59f7f708d862c3ddb0fc07f851e3b8b\n",
            "Best match: nvidia-curand-cu12 10.3.5.147\n",
            "Processing nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-curand-cu12 10.3.5.147 to easy-install.pth file\n",
            "detected new path './nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cufft-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl#sha256=f083fc24912aa410be21fa16d157fed2055dab1cc4b6934a0e03cba69eb242b9\n",
            "Best match: nvidia-cufft-cu12 11.2.1.3\n",
            "Processing nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cufft-cu12 11.2.1.3 to easy-install.pth file\n",
            "detected new path './nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cublas-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl#sha256=2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b\n",
            "Best match: nvidia-cublas-cu12 12.4.5.8\n",
            "Processing nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cublas-cu12 12.4.5.8 to easy-install.pth file\n",
            "detected new path './nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cudnn-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl#sha256=165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n",
            "Best match: nvidia-cudnn-cu12 9.1.0.70\n",
            "Processing nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cudnn-cu12 9.1.0.70 to easy-install.pth file\n",
            "detected new path './nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-cupti-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=9dec60f5ac126f7bb551c055072b69d85392b13311fcc1bcda2202d172df30fb\n",
            "Best match: nvidia-cuda-cupti-cu12 12.4.127\n",
            "Processing nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-cupti-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-runtime-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=64403288fa2136ee8e467cdc9c9427e0434110899d07c779f25b5c068934faa5\n",
            "Best match: nvidia-cuda-runtime-cu12 12.4.127\n",
            "Processing nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-runtime-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-nvrtc-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=a178759ebb095827bd30ef56598ec182b85547f1508941a3d560eb7ea1fbf338\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "Processing nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for torch==2.6.0+cu124\n",
            "Best match: torch 2.6.0+cu124\n",
            "Adding torch 2.6.0+cu124 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for sympy==1.13.1\n",
            "Best match: sympy 1.13.1\n",
            "Adding sympy 1.13.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for triton==3.2.0\n",
            "Best match: triton 3.2.0\n",
            "Adding triton 3.2.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-nvtx-cu12==12.4.127\n",
            "Best match: nvidia-nvtx-cu12 12.4.127\n",
            "Adding nvidia-nvtx-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-nccl-cu12==2.21.5\n",
            "Best match: nvidia-nccl-cu12 2.21.5\n",
            "Adding nvidia-nccl-cu12 2.21.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-cusparselt-cu12==0.6.2\n",
            "Best match: nvidia-cusparselt-cu12 0.6.2\n",
            "Adding nvidia-cusparselt-cu12 0.6.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for fsspec==2025.3.2\n",
            "Best match: fsspec 2025.3.2\n",
            "Adding fsspec 2025.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for jinja2==3.1.6\n",
            "Best match: jinja2 3.1.6\n",
            "Adding jinja2 3.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for networkx==3.4.2\n",
            "Best match: networkx 3.4.2\n",
            "Adding networkx 3.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for typing-extensions==4.13.2\n",
            "Best match: typing-extensions 4.13.2\n",
            "Adding typing-extensions 4.13.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for filelock==3.18.0\n",
            "Best match: filelock 3.18.0\n",
            "Adding filelock 3.18.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Finished processing dependencies for RAdam==0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data_utils(데이터셋)"
      ],
      "metadata": {
        "id": "4z8cYh2wL9u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Union, List\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "class PretrainInputExample:\n",
        "    \"\"\"A single example for unsupervised pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "\n",
        "class ClsInputExample:\n",
        "    \"\"\"A single example for supervised fine-tuning (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str, label: str):\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "class PretrainInputFeatures:\n",
        "    \"\"\"A single set of features of pre-training data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int]):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "class ClsInputFeatures:\n",
        "    \"\"\"A single set of features of fine-tuning data (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int], label_id: int):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 tokenizer,\n",
        "                                 args,\n",
        "                                 mode):\n",
        "    bos_token = tokenizer.bos_token\n",
        "    eos_token = tokenizer.eos_token\n",
        "    pad_token = tokenizer.pad_token\n",
        "\n",
        "    # Build label dict(vocab) with examples\n",
        "    if args.finetune:\n",
        "        if mode == 'train':\n",
        "            labels = sorted(list(set([example.label for example in examples])))\n",
        "            label_dict = {label: i for i, label in enumerate(labels)}\n",
        "            with open(args.cached_label_dict, 'w') as file:\n",
        "                json.dump(label_dict, file,  indent=4)\n",
        "        elif mode == 'test':\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "\n",
        "    # Create features\n",
        "    features = []\n",
        "    for i, example in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "        tokens = [bos_token] + tokens[:args.max_seq_len-2] + [eos_token] # BOS, EOS\n",
        "        tokens += [pad_token] * (args.max_seq_len - len(tokens))\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        if args.finetune:\n",
        "            label_id = label_dict.get(example.label)\n",
        "\n",
        "        if args.pretrain:\n",
        "            feature = PretrainInputFeatures(input_ids)\n",
        "        elif args.finetune:\n",
        "            feature = ClsInputFeatures(input_ids, label_id)\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_examples(args, tokenizer, mode='train'):\n",
        "    if args.local_rank not in [-1, 0]: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    assert mode in ('train', 'test')\n",
        "    cached_features_file = Path('cached_features_{}_{}_{}'.format('pretrain' if args.pretrain else 'finetune', mode, args.max_seq_len))\n",
        "\n",
        "    if cached_features_file.exists():\n",
        "        print('Loading features from cached file', cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        corpus_path = args.train_corpus if mode=='train' else args.test_corpus\n",
        "        with open(corpus_path, 'r', encoding='utf-8') as reader:\n",
        "            corpus = reader.readlines()\n",
        "\n",
        "        # Create examples\n",
        "        if args.pretrain:\n",
        "            corpus = list(map(lambda x: x.strip(), corpus))\n",
        "            corpus = list(filter(lambda x: len(x) > 0, corpus))\n",
        "            examples = [PretrainInputExample(text) for text in corpus]\n",
        "        elif args.finetune:\n",
        "            corpus = list(map(lambda x: x.split('\\t'), corpus))\n",
        "            corpus = list(map(lambda x: list(map(lambda y: y.strip(), x)), corpus))\n",
        "            corpus = list(map(lambda x: list(filter(lambda y: len(y) > 0, x)), corpus))\n",
        "            examples = [ClsInputExample(text, label) for label, text in corpus]\n",
        "\n",
        "        # Convert examples to features\n",
        "        features = convert_examples_to_features(examples, tokenizer, args, mode)\n",
        "\n",
        "        print('Saving features into cached file', cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Create dataset with features\n",
        "    if args.pretrain:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids)\n",
        "    elif args.finetune:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([feature.label_id for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_label_ids)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "10QOGbRAL7bs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tokenization(토큰화)"
      ],
      "metadata": {
        "id": "XHjMUelsO-cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code from\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/translation-transformer/tokenization.py\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/text-classification-transformer/tokenization.py\n",
        "\"\"\"\n",
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.sep_token = sep_token\n",
        "        self.cls_token = cls_token\n",
        "        self.mask_token = mask_token\n",
        "        self.vocab = OrderedDict()\n",
        "        self.ids_to_tokens = OrderedDict()\n",
        "\n",
        "        # Build vocab and ids_to_tokens\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
        "            for i, line in enumerate(reader.readlines()):\n",
        "                token = line.split()[0]\n",
        "                self.vocab[token] = i\n",
        "        for token, id in self.vocab.items():\n",
        "            self.ids_to_tokens[id] = token\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize given text.\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token (str) in an id (integer) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def convert_id_to_token(self, id: int) -> str:\n",
        "        \"\"\"Convert an id (integer) in a token (str) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.ids_to_tokens.get(id, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "        \"\"\"Convert list of tokens in list of ids using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
        "        \"\"\"Convert list of ids in list of tokens using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_id_to_token(id) for id in ids]\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Vocabulary size.\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        \"\"\"Id of pad_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.pad_token)\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self) -> int:\n",
        "        \"\"\"Id of unk_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        \"\"\"Id of bos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.bos_token)\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        \"\"\"Id of eos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.eos_token)\n",
        "\n",
        "    @property\n",
        "    def sep_token_id(self) -> int:\n",
        "        \"\"\"Id of sep_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.sep_token)\n",
        "\n",
        "    @property\n",
        "    def cls_token_id(self) -> int:\n",
        "        \"\"\"Id of cls_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.cls_token)\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self) -> int:\n",
        "        \"\"\"Id of mask_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.mask_token)\n",
        "\n",
        "class PretrainedTokenizer(Tokenizer):\n",
        "    def __init__(self, pretrained_model: str, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        tokenizer = SentencePiece.load(pretrained_model)\n",
        "\n",
        "        super(PretrainedTokenizer, self).__init__(tokenizer, vocab_file, pad_token, unk_token, bos_token, eos_token)\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Detokenize given tokens.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.detokenize(tokens)"
      ],
      "metadata": {
        "id": "MWCsQCspO-M_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 구축(model.py)\n",
        "\n"
      ],
      "metadata": {
        "id": "dxHntuj4XXFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ikuqmz47XFth"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, attn_pdrop):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        # |q| : (batch_size, n_heads, q_len, d_k)\n",
        "        # |k| : (batch_size, n_heads, k_len, d_k)\n",
        "        # |v| : (batch_size, n_heads, v_len, d_v)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
        "        attn_score.masked_fill_(attn_mask, -1e9)\n",
        "        # |attn_score| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "        # |output| : (batch_size, n_heads, q_len, d_v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_pdrop):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model // n_heads\n",
        "\n",
        "        self.WQ = nn.Linear(d_model, d_model)\n",
        "        self.WK = nn.Linear(d_model, d_model)\n",
        "        self.WV = nn.Linear(d_model, d_model)\n",
        "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_pdrop)\n",
        "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # |Q| : (batch_size, q_len(=seq_len), d_model)\n",
        "        # |K| : (batch_size, k_len(=seq_len), d_model)\n",
        "        # |V| : (batch_size, v_len(=seq_len), d_model)\n",
        "        # |attn_mask| : (batch_size, q_len, k_len)\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "        # |q_heads| : (batch_size, n_heads, q_len, d_k)\n",
        "        # |k_heads| : (batch_size, n_heads, k_len, d_k)\n",
        "        # |v_heads| : (batch_size, n_heads, v_len, d_v)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "        # |attn| : (batch_size, n_heads, q_len, d_v)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        # |attn| : (batch_size, q_len, n_heads * d_v)\n",
        "        outputs = self.linear(attn)\n",
        "        # |outputs| : (batch_size, q_len, d_model)\n",
        "\n",
        "        return outputs, attn_weights\n",
        "\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        outputs = self.gelu(self.linear1(inputs))\n",
        "        # |outputs| : (batch_size, seq_len, d_ff)\n",
        "        outputs = self.linear2(outputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def _init__(self, d_model, n_heads, d_ff, attn_pdrop, resid_pdrop):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, attn_pdrop)\n",
        "        self.dropout1 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    ## 입력이 전처리를 거치고, masked 멀티헤드어텐션, 층정규화, feedforward층, 다시 층정규화 거침 ##\n",
        "    ## 중간 중간 잔차 연결 존재 ##\n",
        "    def forward(self, inputs, attn_mask):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "        attn_outputs = self.dropout1(attn_outputs)\n",
        "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "        # |attn_outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len(=seq_len), k_len(=seq_len))\n",
        "\n",
        "        ffn_outputs = self.ffn(attn_outputs)\n",
        "        ffn_outputs = self.dropout2(ffn_outputs)\n",
        "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)  # 잔차 연결\n",
        "        # |ffn_outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return ffn_outputs, attn_weights\n",
        "\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_pdrop, attn_pdrop, resid_pdrop, pad_id):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        # layers\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_pdrop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_pdrop, resid_pdrop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1)+1\n",
        "        position_pad_mask = inputs.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "        # |positions| : (batch_size, seq_len)\n",
        "\n",
        "        ## 포지션 임베딩 값 더해주는 과정 ##\n",
        "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "        # |attn_pad_mask| : (batch_size, seq_len, seq_len)\n",
        "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "        # |subsequent_mask| : (batch_size, seq_len, seq_len)\n",
        "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        ## 디코더 레이어를 n번 통과한 결과 계산 ##\n",
        "        attention_weights = []\n",
        "        for layer in self.layers:\n",
        "            outputs, attn_weights = layer(outputs, attn_mask)\n",
        "            # |outputs| : (batch_size, seq_len, d_model)\n",
        "            # |attn_weights| : (batch_size, n_heads, seq_len, seq_len)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def get_attention_padding_mask(self, q, k, pad_id):\n",
        "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "        # |attn_pad_mask| : (batch_size, q_len, k_len)\n",
        "\n",
        "        return attn_pad_mask\n",
        "\n",
        "    def get_attention_subsequent_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "        # |subsequent_mask| : (batch_size, q_len, q_len)\n",
        "\n",
        "        return subsequent_mask"
      ],
      "metadata": {
        "id": "O3m84tDMY-HA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPT의 기반이 될 모델\n",
        "    - batch개의 시퀀스들을 입력으로 받아 트랜스포머 디코더를 n_layers만큼 순회하는 decoder를 통과한 결과 출력함.\n",
        "    - 이때 outputs => 마지막 디코더의 출력, attention_weights => Query와 Key 행렬곱을 통해 구한 어텐션 스코어에 소프트맥스를 거쳐 나온 결과\n",
        "    - GPT 클래스의 디코더에 해당하는 `TransformerDecoder`에서는 각 디코더 레이어에서의 어텐션 가중치를 배열로 저장해 반환, 최종적으로 `(batch_size, n_head, seq_len, seq_len)`를 n_layer만큼 반복한 크기의 리스트 반환\n",
        "    - 입력을 임베딩 차원으로 확장하고 **포지션 임베딩 값**을 더해주는 과정 적용\n",
        "\n"
      ],
      "metadata": {
        "id": "H9WtRzXVlHpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 사전학습(pre-training) => GPTLMHead\n",
        "- 미세조정(fine-tunong) => GPTClsHead"
      ],
      "metadata": {
        "id": "zfctV6mCq0tI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len=512,\n",
        "                 d_model=768,\n",
        "                 n_layers=12,\n",
        "                 n_heads=12,\n",
        "                 d_ff=3072,\n",
        "                 embd_pdrop=0.1,\n",
        "                 attn_pdrop=0.1,\n",
        "                 resid_pdrop=0.1,\n",
        "                 pad_id=0):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads, d_ff,\n",
        "                                          embd_pdrop, attn_pdrop, resid_pdrop, pad_id)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.decoder(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "\n",
        "## 사전학습(pre-training) ##\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super(GPTLMHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False)  # 마지막 디코더 출력\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.qpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear(outputs)\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "\n",
        "## 미세조정(fine-tuning) ##\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_pdrop=0.1):\n",
        "        super(GPTClsHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "        # Classification\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_pdrop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.qpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear1(outputs)   # 마지막 디코더 출력\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
        "        # |outputs| : (batch_size, d_model)\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "        # |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "        return lm_logits, cls_logits"
      ],
      "metadata": {
        "id": "bfWSq605lGrm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 논문에서는 출력층의 경우, 임베딩 레이어를 마지막 디코더의 출력과 곱해주고 소프트맥스 취하는 방식\n",
        "- 그러나 해당 코드에서는 소프트맥스 취하지 않았음. => loss 계산 시 cross entropy loss 사용하여, 내부적으로 softmax 적용하기 때문에 입력으로 softmax 적용된 값 넣을 필요 없음!"
      ],
      "metadata": {
        "id": "p2d8SPZfs0Vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 훈련(trainer.py)\n",
        "- 만들어진 모델을 거대한 데이터에 대해 학습시킨 후 파인튜닝 시키기 위한 trainer 구현"
      ],
      "metadata": {
        "id": "MUFWjau4vJ0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (1) 사전학습 방법\n",
        "    - 입력과 레이블 정하기. target 데이터 따로 없는 비지도 학습이므로 input 이용하여 target 생성함.\n",
        "\n",
        "- (2) 파인튜닝 방법\n",
        "    - lm_loss는 L_1에 해당, cls_loss는 L_2에 해당.\n",
        "    - ![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "JcvsUBJxOEsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        _args = args[0].args\n",
        "\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "\n",
        "        if _args.distributed:\n",
        "            if _args.local_rank == 0:\n",
        "                print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "        else:\n",
        "            print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "\n",
        "## Trainer ##\n",
        "class Trainer:\n",
        "    def __init__(self, args, train_loader, test_loader, tokenizer):\n",
        "        self.args = args\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.pad_id = tokenizer.pad_token_id\n",
        "        self.eos_id = tokenizer.eos_token_id\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu', args.local_rank)\n",
        "        self.writer = SummaryWriter() if args.local_rank in [-1, 0] else None\n",
        "        self.n_gpus = torch.distributed.get_world_size() if args.distributed else torch.cuda.device_count()\n",
        "        assert args.pretrain != args.finetune\n",
        "        # Do not set both finetune and pretrain arguments to the same (True, False)\n",
        "\n",
        "        if args.pretrained_model:\n",
        "            self.gpt = torch.load(args.pretrained_model)\n",
        "        else:\n",
        "            self.gpt = GPT(vocab_size=self.vocab_size,\n",
        "                           seq_len=args.max_seq_len,\n",
        "                           d_model=args.n_layers,\n",
        "                           n_heads=args.n_attn_heads,\n",
        "                           d_ff=args.fnn_hidden,\n",
        "                           embd_pdrop=args.embd_dropout,\n",
        "                           attn_pdrop=args.attn_dropout,\n",
        "                           resid_pdrop=args.resid_dropout,\n",
        "                           pad_id=self.pad_id)\n",
        "\n",
        "        if args.pretrain:\n",
        "            self.model = GPTLMHead(self.gpt)\n",
        "        if args.finetune:\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "            ## outputs에서 클래스 정보 예측하기 위해 문장 대표하는 한 벡터 추출 ##\n",
        "            ## cls_token_id는 <EOS> 토큰으로 정의됨. ##\n",
        "            ## 논문에서 말한 extract 토큰으로서 역할을 함. ##\n",
        "            self.model = GPTClsHead(self.gpt, n_class=len(label_dict), cls_token_id=self.eos_id)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        if args.distributed:\n",
        "            self.model = DistributedDataParallel(self.model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
        "\n",
        "        self.optimizer = RAdam(self.model.parameters(), args.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=self.pad_id).to(self.device)\n",
        "        self.cls_criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    @timeit\n",
        "    def train(self, epoch):\n",
        "        if self.args.pretrain:\n",
        "            self.pretrain(epoch)\n",
        "        if self.args.finetune:\n",
        "            self.finetune(epoch)\n",
        "\n",
        "    def pretrain(self, epoch):\n",
        "        losses = 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            ## input 이용하여 target 만드는 과정 ##\n",
        "            inputs = batch[0].to(self.device)\n",
        "            targets = inputs[:, 1:].contiguous()\n",
        "            # |inputs| : (batch_size, seq_len), |targets| : (batch_size, seq_len-1)\n",
        "\n",
        "            lm_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size)\n",
        "\n",
        "            loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "            losses += loss.item()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/pre-train', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f}'.format(i, i, n_batches, losses/i))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f}'.format(epoch, self.args.local_rank, losses/n_batches))\n",
        "\n",
        "    def finetune(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset) # n_batches = batch size per GPU\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            ## 위에 수식을 코드로 구현한 부분 ##\n",
        "            inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "            # |inputs| : (batch_size, seq_len), |labels| : (batch_size)\n",
        "\n",
        "            lm_logits, cls_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size), |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "            lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "            cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "            loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "            losses += loss.item()\n",
        "            acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "            accs += acc\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/fine-tune', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                self.writer.add_scalar('Accuracy/fine-tune', acc, ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f} Acc: {:.1f}%'.format(i, i, n_batches, losses/i, accs/i*100.))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.test_loader), len(self.test_loader.dataset)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.test_loader):\n",
        "                if self.args.pretrain:\n",
        "                    inputs = batch.to(self.device)\n",
        "                    targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "                    lm_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "                    losses += loss.item()\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/pre-train(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "\n",
        "                elif self.args.finetune:\n",
        "                    inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "\n",
        "                    lm_logits, cls_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "                    cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "                    loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "                    losses += loss.item()\n",
        "                    acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "                    accs += acc\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/fine-tune(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                        self.writer.add_scalar('Accuracy/fine-tune(eval)', acc, ((epoch-1)*n_batches)+i)\n",
        "\n",
        "        print('Eval Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def save(self, epoch, model_prefix='model', root='.model'):\n",
        "        path = Path(root) / (model_prefix + '.ep%d' % epoch)\n",
        "        if not path.parent.exists():\n",
        "            path.parent.mkdir()\n",
        "\n",
        "        if self.args.distributed:\n",
        "            if self.args.local_rank == 0:\n",
        "                torch.save(self.gpt, path)\n",
        "        else:\n",
        "            torch.save(self.gpt, path)"
      ],
      "metadata": {
        "id": "bPGvCxv9okuc"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}