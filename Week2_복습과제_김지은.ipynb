{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I_mkAk1RGCA2"
      },
      "outputs": [],
      "source": [
        "pip install torch==2.1.0 torchtext==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM9ajlVWGHsL",
        "outputId": "db10afca-5dc2-4527-e8f5-8b57c4ee4df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P66QtYEnH4_S",
        "outputId": "7ac5cb90-2768-44d3-a1ea-d0bd4486ce8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "def generate_tokens(text_iter, language):\n",
        "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
        "\n",
        "    for text in text_iter:\n",
        "        yield token_transform[language](text[language_index[language]])\n",
        "\n",
        "SRC_LANGUAGE = \"de\"\n",
        "TGT_LANGUAGE = \"en\"\n",
        "\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "special_symbols = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "token_transform = {\n",
        "    SRC_LANGUAGE: get_tokenizer(\"spacy\", language=\"de_core_news_sm\"),\n",
        "    TGT_LANGUAGE: get_tokenizer(\"spacy\", language=\"en_core_web_sm\"),\n",
        "}\n",
        "\n",
        "print(\"Token Transform:\")\n",
        "print(token_transform)\n",
        "\n",
        "vocab_transform = {}\n",
        "\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    train_iter = Multi30k(split=\"train\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    vocab_transform[language] = build_vocab_from_iterator(\n",
        "        generate_tokens(train_iter, language),\n",
        "        min_freq=1,\n",
        "        specials=special_symbols,\n",
        "        special_first=True,\n",
        "    )\n",
        "\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    vocab_transform[language].set_default_index(UNK_IDX)\n",
        "\n",
        "print(\"Vocab Transform:\")\n",
        "print(vocab_transform)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhhxysVqGIFc",
        "outputId": "f4929f5b-f738-4b7f-aede-04852f8bfbcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Transform:\n",
            "{'de': functools.partial(<function _spacy_tokenize at 0x7fd50a61fec0>, spacy=<spacy.lang.de.German object at 0x7fd4f5daad50>), 'en': functools.partial(<function _spacy_tokenize at 0x7fd50a61fec0>, spacy=<spacy.lang.en.English object at 0x7fd4f4999110>)}\n",
            "Vocab Transform:\n",
            "{'de': Vocab(), 'en': Vocab()}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_encoder_layers,\n",
        "        num_decoder_layers,\n",
        "        emb_size,\n",
        "        max_len,\n",
        "        nhead,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        dim_feedforward,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            d_model=emb_size, max_len=max_len, dropout=dropout\n",
        "        )\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=emb_size,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        trg,\n",
        "        src_mask,\n",
        "        tgt_mask,\n",
        "        src_padding_mask,\n",
        "        tgt_padding_mask,\n",
        "        memory_key_padding_mask,\n",
        "    ):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=None,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask,\n",
        "        )\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.transformer.encoder(\n",
        "            self.positional_encoding(self.src_tok_emb(src)), src_mask\n",
        "        )\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask):\n",
        "        return self.transformer.decoder(\n",
        "            self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask\n",
        "        )"
      ],
      "metadata": {
        "id": "Secxsmq-GJTB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = torch.nn.Transformer(\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_encoder_layers=6,\n",
        "    num_decoder_layers=6,\n",
        "    dim_feedforward=2048,\n",
        "    dropout=0.1,\n",
        "    activation=torch.nn.functional.relu,\n",
        "    layer_norm_eps=1e-05,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_spIwGoeGK5I",
        "outputId": "c8da077d-5821-4daf-8920-8e7f5cadd8be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = transformer.forward(\n",
        "    src,\n",
        "    tgt,\n",
        "    src_mask=None,\n",
        "    tgt_mask=None,\n",
        "    memory_mask=None,\n",
        "    src_key_padding_mask=None,\n",
        "    tgt_key_padding_mask=None,\n",
        "    memory_key_padding_mask=None,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "5WvV5lcwGUgw",
        "outputId": "689c39f8-0de9-4607-dfa5-c0ee5ef0af6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'src' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-893d12bf6447>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m output = transformer.forward(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtgt_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'src' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    num_encoder_layers=3,\n",
        "    num_decoder_layers=3,\n",
        "    emb_size=512,\n",
        "    max_len=512,\n",
        "    nhead=8,\n",
        "    src_vocab_size=len(vocab_transform[SRC_LANGUAGE]),\n",
        "    tgt_vocab_size=len(vocab_transform[TGT_LANGUAGE]),\n",
        "    dim_feedforward=512,\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for main_name, main_module in model.named_children():\n",
        "    print(main_name)\n",
        "    for sub_name, sub_module in main_module.named_children():\n",
        "        print(\"|__\", sub_name)\n",
        "        for ssub_name, ssub_module in sub_module.named_children():\n",
        "            print(\"|  |__\", ssub_name)\n",
        "            for sssub_name, sssub_module in ssub_module.named_children():\n",
        "                print(\"|  |  |__\", sssub_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNNmXSWVGaSY",
        "outputId": "d260a0a6-eea2-4c52-fc6d-136d28cebe32"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_tok_emb\n",
            "|__ embedding\n",
            "tgt_tok_emb\n",
            "|__ embedding\n",
            "positional_encoding\n",
            "|__ dropout\n",
            "transformer\n",
            "|__ encoder\n",
            "|  |__ layers\n",
            "|  |  |__ 0\n",
            "|  |  |__ 1\n",
            "|  |  |__ 2\n",
            "|  |__ norm\n",
            "|__ decoder\n",
            "|  |__ layers\n",
            "|  |  |__ 0\n",
            "|  |  |__ 1\n",
            "|  |  |__ 2\n",
            "|  |__ norm\n",
            "generator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "def input_transform(token_ids):\n",
        "    return torch.cat(\n",
        "        (torch.tensor([BOS_IDX]), torch.tensor(token_ids), torch.tensor([EOS_IDX]))\n",
        "    )\n",
        "\n",
        "def collator(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch\n",
        "\n",
        "text_transform = {}\n",
        "for language in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[language] = sequential_transforms(\n",
        "        token_transform[language],\n",
        "        vocab_transform[language],\n",
        "        input_transform\n",
        "    )\n",
        "\n",
        "data_iter = Multi30k(split=\"valid\", language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "\n",
        "source_tensor, target_tensor = next(iter(dataloader))\n",
        "\n",
        "print(\"source_batch:\", source_tensor.shape)\n",
        "print(\"source_tensor:\", source_tensor)\n",
        "\n",
        "print(\"target_batch:\", target_tensor.shape)\n",
        "print(\"target_tensor:\", target_tensor)"
      ],
      "metadata": {
        "id": "cMABctOjGkWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b679af-8bac-4ab7-d660-9c4abbc1e06a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "source_batch: torch.Size([35, 128])\n",
            "source_tensor: tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [  14,    5,    5,  ...,    5,   21,    5],\n",
            "        [  38,   12,   35,  ...,   12, 1750,   69],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n",
            "target_batch: torch.Size([30, 128])\n",
            "target_tensor: tensor([[   2,    2,    2,  ...,    2,    2,    2],\n",
            "        [   6,    6,    6,  ...,  250,   19,    6],\n",
            "        [  39,   12,   35,  ...,   12, 3254,   61],\n",
            "        ...,\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1],\n",
            "        [   1,    1,    1,  ...,    1,    1,    1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = torch.triu(torch.ones((sz, sz), device=DEVICE), diagonal=1).bool()\n",
        "    return mask  # now it's boolean\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len), device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "id": "_I7UZmsuGyBT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(model, optimizer, criterion, split):\n",
        "    model.train() if split == \"train\" else model.eval()\n",
        "\n",
        "    data_iter = Multi30k(split=split, language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    dataloader = DataLoader(data_iter, batch_size=BATCH_SIZE, collate_fn=collator)\n",
        "\n",
        "    losses = 0\n",
        "\n",
        "    for source_batch, target_batch in dataloader:\n",
        "        source_batch = source_batch.to(DEVICE)\n",
        "        target_batch = target_batch.to(DEVICE)\n",
        "\n",
        "        target_input = target_batch[:-1, :]\n",
        "        target_output = target_batch[1:, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(\n",
        "            source_batch, target_input\n",
        "        )\n",
        "\n",
        "        logits = model(\n",
        "            src=source_batch,\n",
        "            trg=target_input,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_padding_mask=src_padding_mask,\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask,\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(logits.reshape(-1, logits.shape[-1]), target_output.reshape(-1))\n",
        "\n",
        "        if split == \"train\":\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(dataloader))\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = run(model, optimizer, criterion, \"train\")\n",
        "    val_loss = run(model, optimizer, criterion, \"valid\")\n",
        "    print(f\"Epoch: {epoch+1}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")"
      ],
      "metadata": {
        "id": "OoG_vYnkHKE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_decode(model, source_tensor, source_mask, max_len, start_symbol):\n",
        "    source_tensor = source_tensor.to(DEVICE)\n",
        "    source_mask = source_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(source_tensor, source_mask)\n",
        "    memory = memory.to(DEVICE)\n",
        "\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        target_mask = generate_square_subsequent_mask(ys.size(0))\n",
        "        target_mask = target_mask.type(torch.bool).to(DEVICE)\n",
        "\n",
        "        out = model.decode(ys, memory, target_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat(\n",
        "            [ys, torch.ones(1, 1).type_as(source_tensor.data).fill_(next_word)], dim=0\n",
        "        )\n",
        "\n",
        "        if next_word == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n",
        "\n",
        "\n",
        "def translate(model, source_sentence):\n",
        "    model.eval()\n",
        "\n",
        "    source_tensor = text_transform[SRC_LANGUAGE](source_sentence).view(-1, 1)\n",
        "    num_tokens = source_tensor.shape[0]\n",
        "\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,\n",
        "        source_tensor,\n",
        "        src_mask,\n",
        "        max_len=num_tokens + 5,\n",
        "        start_symbol=BOS_IDX,\n",
        "    ).flatten()\n",
        "\n",
        "    output = vocab_transform[TGT_LANGUAGE].lookup_tokens(\n",
        "        list(tgt_tokens.cpu().numpy())\n",
        "    )[1:-1]  # BOS, EOS 제거\n",
        "\n",
        "    return \" \".join(output)\n",
        "\n",
        "\n",
        "output_oov = translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\")\n",
        "output = translate(model, \"Eine Gruppe von Menschen steht vor einem Gebäude .\")\n",
        "\n",
        "print(output_oov)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "eRbQxXbPHt8K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}