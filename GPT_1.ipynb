{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdWl8b3var9nQfth/WQFRL"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## data_utils.py"
      ],
      "metadata": {
        "id": "fl4qmCYo9Z_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Union, List\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "class PretrainInputExample:\n",
        "    \"\"\"A single example for unsupervised pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "\n",
        "class ClsInputExample:\n",
        "    \"\"\"A single example for supervised fine-tuning (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str, label: str):\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "class PretrainInputFeatures:\n",
        "    \"\"\"A single set of features of pre-training data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int]):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "class ClsInputFeatures:\n",
        "    \"\"\"A single set of features of fine-tuning data (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int], label_id: int):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 tokenizer,\n",
        "                                 args,\n",
        "                                 mode):\n",
        "    bos_token = tokenizer.bos_token\n",
        "    eos_token = tokenizer.eos_token\n",
        "    pad_token = tokenizer.pad_token\n",
        "\n",
        "    # Build label dict(vocab) with examples\n",
        "    if args.finetune:\n",
        "        if mode == 'train':\n",
        "            labels = sorted(list(set([example.label for example in examples])))\n",
        "            label_dict = {label: i for i, label in enumerate(labels)}\n",
        "            with open(args.cached_label_dict, 'w') as file:\n",
        "                json.dump(label_dict, file,  indent=4)\n",
        "        elif mode == 'test':\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "\n",
        "    # Create features\n",
        "    features = []\n",
        "    for i, example in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "        tokens = [bos_token] + tokens[:args.max_seq_len-2] + [eos_token] # BOS, EOS\n",
        "        tokens += [pad_token] * (args.max_seq_len - len(tokens))\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        if args.finetune:\n",
        "            label_id = label_dict.get(example.label)\n",
        "\n",
        "        if args.pretrain:\n",
        "            feature = PretrainInputFeatures(input_ids)\n",
        "        elif args.finetune:\n",
        "            feature = ClsInputFeatures(input_ids, label_id)\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_examples(args, tokenizer, mode='train'):\n",
        "    if args.local_rank not in [-1, 0]: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    assert mode in ('train', 'test')\n",
        "    cached_features_file = Path('cached_features_{}_{}_{}'.format('pretrain' if args.pretrain else 'finetune', mode, args.max_seq_len))\n",
        "\n",
        "    if cached_features_file.exists():\n",
        "        print('Loading features from cached file', cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        corpus_path = args.train_corpus if mode=='train' else args.test_corpus\n",
        "        with open(corpus_path, 'r', encoding='utf-8') as reader:\n",
        "            corpus = reader.readlines()\n",
        "\n",
        "        # Create examples\n",
        "        if args.pretrain:\n",
        "            corpus = list(map(lambda x: x.strip(), corpus))\n",
        "            corpus = list(filter(lambda x: len(x) > 0, corpus))\n",
        "            examples = [PretrainInputExample(text) for text in corpus]\n",
        "        elif args.finetune:\n",
        "            corpus = list(map(lambda x: x.split('\\t'), corpus))\n",
        "            corpus = list(map(lambda x: list(map(lambda y: y.strip(), x)), corpus))\n",
        "            corpus = list(map(lambda x: list(filter(lambda y: len(y) > 0, x)), corpus))\n",
        "            examples = [ClsInputExample(text, label) for label, text in corpus]\n",
        "\n",
        "        # Convert examples to features\n",
        "        features = convert_examples_to_features(examples, tokenizer, args, mode)\n",
        "\n",
        "        print('Saving features into cached file', cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Create dataset with features\n",
        "    if args.pretrain:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids)\n",
        "    elif args.finetune:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([feature.label_id for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_label_ids)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "QM3UzIdo9XRX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.py"
      ],
      "metadata": {
        "id": "th1PxX1R9bJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, attn_pdrop):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        # |q| : (batch_size, n_heads, q_len, d_k)\n",
        "        # |k| : (batch_size, n_heads, k_len, d_k)\n",
        "        # |v| : (batch_size, n_heads, v_len, d_v)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
        "        attn_score.masked_fill_(attn_mask, -1e9)\n",
        "        # |attn_scroe| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "        # |output| : (batch_size, n_heads, q_len, d_v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_pdrop):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "\n",
        "        self.WQ = nn.Linear(d_model, d_model)\n",
        "        self.WK = nn.Linear(d_model, d_model)\n",
        "        self.WV = nn.Linear(d_model, d_model)\n",
        "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_pdrop)\n",
        "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # |Q| : (batch_size, q_len(=seq_len), d_model)\n",
        "        # |K| : (batch_size, k_len(=seq_len), d_model)\n",
        "        # |V| : (batch_size, v_len(=seq_len), d_model)\n",
        "        # |attn_mask| : (batch_size, q_len, k_len)\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "        # |q_heads| : (batch_size, n_heads, q_len, d_k), |k_heads| : (batch_size, n_heads, k_len, d_k), |v_heads| : (batch_size, n_heads, v_len, d_v)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "        # |attn| : (batch_size, n_heads, q_len, d_v)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        # |attn| : (batch_size, q_len, n_heads * d_v)\n",
        "        outputs = self.linear(attn)\n",
        "        # |outputs| : (batch_size, q_len, d_model)\n",
        "\n",
        "        return outputs, attn_weights\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        outputs = self.gelu(self.linear1(inputs))\n",
        "        # |outputs| : (batch_size, seq_len, d_ff)\n",
        "        outputs = self.linear2(outputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, attn_pdrop, resid_pdrop):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, attn_pdrop)\n",
        "        self.dropout1 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, inputs, attn_mask):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "        attn_outputs = self.dropout1(attn_outputs)\n",
        "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "        # |attn_outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len(=seq_len), k_len(=seq_len))\n",
        "\n",
        "        ffn_outputs = self.ffn(attn_outputs)\n",
        "        ffn_outputs = self.dropout2(ffn_outputs)\n",
        "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
        "        # |ffn_outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return ffn_outputs, attn_weights\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_pdrop, attn_pdrop, resid_pdrop, pad_id):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        # layers\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_pdrop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_pdrop, resid_pdrop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
        "        position_pad_mask = inputs.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "        # |positions| : (batch_size, seq_len)\n",
        "\n",
        "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions) # 입력을 임베딩 차원으로 확장 -> 포지션 임베딩 값 더하기\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "        # |attn_pad_mask| : (batch_size, seq_len, seq_len)\n",
        "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "        # |subsequent_mask| : (batch_size, seq_len, seq_len)\n",
        "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        # 디코더 레이어를 n번 통과한 결과를 계산\n",
        "        attention_weights = []\n",
        "        for layer in self.layers:\n",
        "            outputs, attn_weights = layer(outputs, attn_mask)\n",
        "            # |outputs| : (batch_size, seq_len, d_model)\n",
        "            # |attn_weights| : (batch_size, n_heads, seq_len, seq_len)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def get_attention_padding_mask(self, q, k, pad_id):\n",
        "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "        # |attn_pad_mask| : (batch_size, q_len, k_len)\n",
        "\n",
        "        return attn_pad_mask\n",
        "\n",
        "    def get_attention_subsequent_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "        # |subsequent_mask| : (batch_size, q_len, q_len)\n",
        "\n",
        "        return subsequent_mask\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len=512,\n",
        "                 d_model=768,\n",
        "                 n_layers=12,\n",
        "                 n_heads=12,\n",
        "                 d_ff=3072,\n",
        "                 embd_pdrop=0.1,\n",
        "                 attn_pdrop=0.1,\n",
        "                 resid_pdrop=0.1,\n",
        "                 pad_id=0):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads, d_ff,\n",
        "                                          embd_pdrop, attn_pdrop, resid_pdrop, pad_id)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.decoder(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super(GPTLMHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear(outputs)\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_pdrop=0.1):\n",
        "        super(GPTClsHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "        # Classification\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_pdrop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear1(outputs)\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
        "        # |outputs| : (batch_size, d_model)\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "        # |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "        return lm_logits, cls_logits"
      ],
      "metadata": {
        "id": "S44U-R5O9X2c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- batch개의 시퀀스를 입력\n",
        "- 트랜스포머 디코더를 n_layer만큼 순회하는 decoder를 통과한 결과 출력\n",
        "\n",
        "`ScaledDotProductAttention`\n",
        "- attention_weights: Query와 Key의 행렬곱 -> 어텐션 스코어 -> 소프트맥스 거친 결과\n",
        "- output: 마지막 디코더 출력"
      ],
      "metadata": {
        "id": "cIjYDwSF9_Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tokenization.py"
      ],
      "metadata": {
        "id": "YQ5r1w-N9h4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Code from\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/translation-transformer/tokenization.py\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/text-classification-transformer/tokenization.py\n",
        "\"\"\"\n",
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.sep_token = sep_token\n",
        "        self.cls_token = cls_token\n",
        "        self.mask_token = mask_token\n",
        "        self.vocab = OrderedDict()\n",
        "        self.ids_to_tokens = OrderedDict()\n",
        "\n",
        "        # Build vocab and ids_to_tokens\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
        "            for i, line in enumerate(reader.readlines()):\n",
        "                token = line.split()[0]\n",
        "                self.vocab[token] = i\n",
        "        for token, id in self.vocab.items():\n",
        "            self.ids_to_tokens[id] = token\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize given text.\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token (str) in an id (integer) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def convert_id_to_token(self, id: int) -> str:\n",
        "        \"\"\"Convert an id (integer) in a token (str) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.ids_to_tokens.get(id, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "        \"\"\"Convert list of tokens in list of ids using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
        "        \"\"\"Convert list of ids in list of tokens using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_id_to_token(id) for id in ids]\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Vocabulary size.\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        \"\"\"Id of pad_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.pad_token)\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self) -> int:\n",
        "        \"\"\"Id of unk_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        \"\"\"Id of bos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.bos_token)\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        \"\"\"Id of eos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.eos_token)\n",
        "\n",
        "    @property\n",
        "    def sep_token_id(self) -> int:\n",
        "        \"\"\"Id of sep_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.sep_token)\n",
        "\n",
        "    @property\n",
        "    def cls_token_id(self) -> int:\n",
        "        \"\"\"Id of cls_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.cls_token)\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self) -> int:\n",
        "        \"\"\"Id of mask_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.mask_token)\n",
        "\n",
        "class PretrainedTokenizer(Tokenizer):\n",
        "    def __init__(self, pretrained_model: str, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        tokenizer = SentencePiece.load(pretrained_model)\n",
        "\n",
        "        super(PretrainedTokenizer, self).__init__(tokenizer, vocab_file, pad_token, unk_token, bos_token, eos_token)\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Detokenize given tokens.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.detokenize(tokens)"
      ],
      "metadata": {
        "id": "Z7EAdcMC9j-u"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trainer.py\n",
        "파인튜닝 시키기 위한 trainer"
      ],
      "metadata": {
        "id": "USvdL7gM9pgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from radam import RAdam\n",
        "\n",
        "from model import GPT, GPTLMHead, GPTClsHead\n",
        "\n",
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        _args = args[0].args\n",
        "\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "\n",
        "        if _args.distributed:\n",
        "            if _args.local_rank == 0:\n",
        "                print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "        else:\n",
        "            print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args, train_loader, test_loader, tokenizer):\n",
        "        self.args = args\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.pad_id = tokenizer.pad_token_id\n",
        "        self.eos_id = tokenizer.eos_token_id\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu', args.local_rank)\n",
        "        self.writer = SummaryWriter() if args.local_rank in [-1, 0] else None\n",
        "        self.n_gpus = torch.distributed.get_world_size() if args.distributed else torch.cuda.device_count()\n",
        "        assert args.pretrain != args.finetune # Do not set both finetune and pretrain arguments to the same (True, False)\n",
        "\n",
        "        if args.pretrained_model:\n",
        "            self.gpt = torch.load(args.pretrained_model)\n",
        "        else:\n",
        "            self.gpt = GPT(vocab_size=self.vocab_size,\n",
        "                           seq_len=args.max_seq_len,\n",
        "                           d_model=args.hidden,\n",
        "                           n_layers=args.n_layers,\n",
        "                           n_heads=args.n_attn_heads,\n",
        "                           d_ff=args.ffn_hidden,\n",
        "                           embd_pdrop=args.embd_dropout,\n",
        "                           attn_pdrop=args.attn_dropout,\n",
        "                           resid_pdrop=args.resid_dropout,\n",
        "                           pad_id=self.pad_id)\n",
        "\n",
        "        if args.pretrain:\n",
        "            self.model = GPTLMHead(self.gpt)\n",
        "            self.model.to(self.device)\n",
        "        if args.finetune:\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "            self.model = GPTClsHead(self.gpt, n_class=len(label_dict), cls_token_id=self.eos_id)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        if args.distributed:\n",
        "            self.model = DistributedDataParallel(self.model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
        "\n",
        "        self.optimizer = RAdam(self.model.parameters(), args.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index = self.pad_id).to(self.device)\n",
        "        self.cls_criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    @timeit\n",
        "    def train(self, epoch):\n",
        "        if self.args.pretrain:\n",
        "            self.pretrain(epoch)\n",
        "        if self.args.finetune:\n",
        "            self.finetune(epoch)\n",
        "\n",
        "    def pretrain(self, epoch):\n",
        "        losses = 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs = batch[0].to(self.device)\n",
        "            targets = inputs[:, 1:].contiguous()\n",
        "            # |inputs| : (batch_size, seq_len), |targets| : (batch_size, seq_len-1)\n",
        "\n",
        "            lm_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size)\n",
        "\n",
        "            loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "            losses += loss.item()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/pre-train', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f}'.format(i, i, n_batches, losses/i))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f}'.format(epoch, self.args.local_rank, losses/n_batches))\n",
        "\n",
        "    def finetune(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset) # n_batches = batch size per GPU\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "            # |inputs| : (batch_size, seq_len), |labels| : (batch_size)\n",
        "\n",
        "            lm_logits, cls_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size), |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "            lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "            cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "            loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "            losses += loss.item()\n",
        "            acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "            accs += acc\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/fine-tune', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                self.writer.add_scalar('Accuracy/fine-tune', acc, ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f} Acc: {:.1f}%'.format(i, i, n_batches, losses/i, accs/i*100.))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.test_loader), len(self.test_loader.dataset)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.test_loader):\n",
        "                if self.args.pretrain:\n",
        "                    inputs = batch.to(self.device)\n",
        "                    targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "                    lm_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "                    losses += loss.item()\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/pre-train(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "\n",
        "                elif self.args.finetune:\n",
        "                    inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "\n",
        "                    lm_logits, cls_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "                    cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "                    loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "                    losses += loss.item()\n",
        "                    acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "                    accs += acc\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/fine-tune(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                        self.writer.add_scalar('Accuracy/fine-tune(eval)', acc, ((epoch-1)*n_batches)+i)\n",
        "\n",
        "        print('Eval Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def save(self, epoch, model_prefix='model', root='.model'):\n",
        "        path = Path(root) / (model_prefix + '.ep%d' % epoch)\n",
        "        if not path.parent.exists():\n",
        "            path.parent.mkdir()\n",
        "\n",
        "        if self.args.distributed:\n",
        "            if self.args.local_rank == 0:\n",
        "                torch.save(self.gpt, path)\n",
        "        else:\n",
        "            torch.save(self.gpt, path)"
      ],
      "metadata": {
        "id": "Xz4kWpTz9gal"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}