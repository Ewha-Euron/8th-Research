{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zyRbYpFTAkIY"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 미로 찾기 : 에이전트가 움직이면서 목표 지점 (4, 6)에 도달하고자 함\n",
        "class GridWorld():\n",
        "    def __init__(self):\n",
        "        self.x=0\n",
        "        self.y=0\n",
        "\n",
        "    def step(self, a): # 새로운 액션 a 수행\n",
        "        # 0번 액션: 왼쪽, 1번 액션: 위, 2번 액션: 오른쪽, 3번 액션: 아래쪽\n",
        "        if a==0:\n",
        "            self.move_left()\n",
        "        elif a==1:\n",
        "            self.move_up()\n",
        "        elif a==2:\n",
        "            self.move_right()\n",
        "        elif a==3:\n",
        "            self.move_down()\n",
        "\n",
        "        reward = -1 # 보상은 항상 -1로 고정 -> 최대한 빠르게 목표 도달 목적\n",
        "        done = self.is_done() # done : 목표 도달 여부( True / False )\n",
        "        return (self.x, self.y), reward, done # (새로운 x, 새로운 y), 보상, 목표 도달 여부\n",
        "\n",
        "    def move_left(self): # 0번 액션 : 왼쪽 방향\n",
        "        if self.y==0: #( . 0) 이동 금지\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [0,1,2]: #(0~2, 3) 이동 금지\n",
        "            pass\n",
        "        elif self.y==5 and self.x in [2,3,4]: #(2~4, 5) 이동 금지\n",
        "            pass\n",
        "        else:\n",
        "            self.y -= 1 # 나머지 경우는 모두 이동 가능\n",
        "\n",
        "    def move_right(self): # 2번 액션 : 오른쪽 방향\n",
        "        if self.y==1 and self.x in [0,1,2]:\n",
        "            pass\n",
        "        elif self.y==3 and self.x in [2,3,4]:\n",
        "            pass\n",
        "        elif self.y==6:\n",
        "            pass\n",
        "        else:\n",
        "            self.y += 1\n",
        "\n",
        "    def move_up(self): # 1번 액션 : 위쪽 방향\n",
        "        if self.x==0:\n",
        "            pass\n",
        "        elif self.x==3 and self.y==2:\n",
        "            pass\n",
        "        else:\n",
        "            self.x -= 1\n",
        "\n",
        "    def move_down(self): # 3번 액션 : 아래쪽 방향\n",
        "        if self.x==4:\n",
        "            pass\n",
        "        elif self.x==1 and self.y==4:\n",
        "            pass\n",
        "        else:\n",
        "            self.x+=1\n",
        "\n",
        "    def is_done(self):\n",
        "        if self.x==4 and self.y==6: # 현재 위치가 (4, 6) 경우 목표 도달\n",
        "            return True # True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def reset(self): # 초기화 부분\n",
        "        self.x = 0\n",
        "        self.y = 0\n",
        "        return (self.x, self.y)"
      ],
      "metadata": {
        "id": "lDNo5qTwA9yj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GridWorld에서 Q-learning으로 최적의 정책 학습\n",
        "class QAgent():\n",
        "    def __init__(self):\n",
        "        self.q_table = np.zeros((5, 7, 4)) # Q 테이블을 0으로 초기화\n",
        "                                           # Q-table : (행 위치 x, 열 위치 y, 행동 a)\n",
        "        self.eps = 0.9 # 입실론값 초기값 : 0.9 -> 90% 확률로 탐험 / 10% 확률로 greedy 이용\n",
        "\n",
        "    # 입실론- greedy 정책으로 액션 선택\n",
        "    def select_action(self, s):\n",
        "        # eps-greedy로 액션을 선택해준다\n",
        "        x, y = s # s : 현재 상태\n",
        "        coin = random.random()\n",
        "        if coin < self.eps:\n",
        "            action = random.randint(0,3) # 랜덤으로 약션 탐험\n",
        "        else:\n",
        "            action_val = self.q_table[x,y,:]\n",
        "            action = np.argmax(action_val)\n",
        "        return action\n",
        "\n",
        "    # Q-learning으로 테이블 업데이트\n",
        "    def update_table(self, transition):\n",
        "        s, a, r, s_prime = transition\n",
        "        x,y = s\n",
        "        next_x, next_y = s_prime\n",
        "        a_prime = self.select_action(s_prime) # S'에서 선택할 액션 (실제로 취한 액션이 아님)\n",
        "        # Q러닝 업데이트 식을 이용\n",
        "        self.q_table[x,y,a] = self.q_table[x,y,a] + 0.1 * (r + np.amax(self.q_table[next_x,next_y,:]) - self.q_table[x,y,a]) # a : 학습률\n",
        "\n",
        "    # 탐험 -> 이용\n",
        "    def anneal_eps(self):\n",
        "        self.eps -= 0.01  # Q러닝에선 epsilon 이 좀더 천천히 줄어 들도록 함. -> 탐험에서 이용 중심으로\n",
        "        self.eps = max(self.eps, 0.2)\n",
        "\n",
        "    # 최종 학습 결과 출력\n",
        "    def show_table(self):\n",
        "        q_lst = self.q_table.tolist()\n",
        "        data = np.zeros((5,7))\n",
        "        for row_idx in range(len(q_lst)): # 각 위치에서 가장 Q값이 큰 액션 출력\n",
        "            row = q_lst[row_idx]\n",
        "            for col_idx in range(len(row)):\n",
        "                col = row[col_idx]\n",
        "                action = np.argmax(col)\n",
        "                data[row_idx, col_idx] = action\n",
        "        print(data)"
      ],
      "metadata": {
        "id": "nZ7iGQakBCy8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 메인 루프\n",
        "def main():\n",
        "    env = GridWorld() # GridWorld 환경 초기화\n",
        "    agent = QAgent() # QAgent 즉 에이전트 초기화\n",
        "\n",
        "    # 에피소드 반복\n",
        "    for n_epi in range(1000): # 에피소드 1000개 반복 (각각 (0, 0) -> (4, 6))\n",
        "        done = False\n",
        "\n",
        "        # 환경 초기화 & 상태 리셋\n",
        "        s = env.reset()\n",
        "        # done이 아님 => 목표 상태 도달 X\n",
        "        while not done:\n",
        "            a = agent.select_action(s) # 액션 선택\n",
        "            s_prime, r, done = env.step(a) # 액션 수행하고 다음 상태, 보상, 목표 도달 여부\n",
        "            agent.update_table((s,a,r,s_prime)) # Q-table 업데이트\n",
        "            s = s_prime\n",
        "        agent.anneal_eps() # 탐험 줄임 -> 점점 학습된 정책 따르도록 함\n",
        "\n",
        "    agent.show_table()"
      ],
      "metadata": {
        "id": "m5JS-76zFCEo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W6KS49IFlpN",
        "outputId": "9d1b6f4a-38c8-46c0-f702-a596b773217e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2. 3. 0. 2. 3. 3. 3.]\n",
            " [3. 3. 0. 2. 2. 3. 3.]\n",
            " [3. 3. 0. 1. 0. 3. 3.]\n",
            " [2. 2. 2. 1. 0. 3. 3.]\n",
            " [3. 1. 1. 1. 0. 2. 0.]]\n"
          ]
        }
      ]
    }
  ]
}