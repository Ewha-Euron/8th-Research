# **논문 분석: Deep Residual Learning for Image Recognition (2016, Kaiming He et al.)**

과제 공부 기록 : [은체공부 티스토리](https://junggoldchae-coding.tistory.com/entry/✳%EF%B8%8F-Deep-residual-learning-for-image-recognition-논문-리뷰-ing)

---

## **1. Introduction**

### 💡 **논문 주제 및 필요성**  

이 논문은 잔차 학습(Residual Learning)을 활용하여 매우 깊은 신경망(Deep Neural Networks)을 학습할 때 발생하는 퇴화 문제(Degradation Problem)를 해결하고자 합니다.

- **주제:**  
  이미지 인식(Image Recognition)에서 매우 깊은 신경망의 학습을 용이하게 만드는 잔차 학습 프레임워크(Residual Learning Framework)를 제안합니다.

- **필요성:**  
  기존의 신경망은 깊이를 늘리면 **기울기 소실(Vanishing Gradient)** 문제는 해결되었지만,  
  여전히 **깊이가 증가할수록 학습 오류가 높아지는 퇴화 문제**가 존재했습니다.  
  이는 신경망의 학습 가능 깊이를 제한하는 주요 요인이었습니다.

---

### 💡 **기존 방법의 문제점**  

- 기존의 깊은 신경망에서는 학습 오류(Training Error)가 네트워크 깊이 증가에 따라 오히려 **증가**하는 문제가 있었습니다.  
- 이러한 퇴화 문제는 단순히 **과적합(Overfitting)** 문제가 아니라,  
  **신경망이 정체 매핑(Identity Mapping)을 학습하지 못하기 때문**임을 지적했습니다.

---

### 💡 **논문에서 제안하는 방법**  

- 이 논문은 잔차 학습(Residual Learning)을 통해,  
  신경망이 직접 목표 함수를 학습하는 대신,  
  잔차 함수(residual function)를 학습하도록 합니다.  
- 이를 통해 깊은 네트워크에서도  
  학습 난이도(ease of learning)를 낮추고,  
  **기울기 소실 문제**를 완화하여 학습을 원활하게 합니다.

---

### **논문이 다루는 분야**  

- **이미지 인식(Image Recognition)**  
- **컴퓨터 비전(Computer Vision)**  
- **딥러닝 모델 최적화(Deep Learning Optimization)**  

---

### **기존 연구의 한계점**  

- 기존의 **VGGNet**, **GoogLeNet**과 같은 모델들은 **깊이를 늘리는 방식**으로 성능을 높였습니다.  
- 하지만 깊이가 증가할수록 **학습 오류**가 증가하는 **퇴화 문제**가 발생했습니다.  
- 이는 기존의 **비잔차 학습(Plain Learning)** 방식으로는 깊은 네트워크를 효과적으로 학습할 수 없음을 의미했습니다.

---

### **논문의 Contributions**  

1. 잔차 학습 프레임워크 제안:
   - 네트워크가 **잔차(residual)**를 학습하도록 하여 깊은 네트워크에서도 **학습 안정성**을 유지할 수 있게 했습니다.

2. **지름길 연결(Shortcut Connection)의 도입:**  
   - 추가적인 **매개변수(parameter)**나 **계산 복잡도(computational complexity)** 없이  
     신호가 레이어를 통과하도록 설계하여 **기울기 보존(Gradient Preservation)**을 가능하게 했습니다.

3. **실험적 증거 제시:**  
   - **ImageNet**, **CIFAR-10** 데이터셋에서  
     **잔차 네트워크(ResNet)**가 기존 모델을 능가하는 성능을 보여주었습니다.

---

## **2. Related Work**

### 💡 **기존 연구에 대한 서술**  

- 기존 연구에서는 **VGGNet**, **GoogLeNet**과 같은 **매우 깊은 신경망(very deep networks)**이  
  이미지 인식에서 좋은 성과를 보였다고 언급합니다.  
- 하지만 이러한 모델들은 **정체 매핑을 학습하지 못해** 깊이를 늘리면 오히려 **성능이 저하**되는 한계가 있었습니다.

---

### 💡 **제안 방법의 차별성**  

- **잔차 학습(Residual Learning)**을 통해  
  신경망이 **잔차 함수(residual function)**를 학습하게 하여  
  기존 모델들의 **학습 오류 증가 문제**를 해결했습니다.  
- 특히, **하이웨이 네트워크(Highway Networks)**와 비교했을 때  
  **게이트(gate)**가 없는 **정체 지름길(identity shortcut)**을 사용하여  
  항상 **잔차 함수를 학습**하게 만든 것이 차별점입니다.

---

## **3. 제안 방법론**

### 💡 **일관성 및 구현 가능성**  

- 논문은 **Introduction**에서 제시한 **잔차 학습의 필요성**을  
  **구체적인 방법론(section 3)**에서 일관되게 설명하고 있습니다.  
- **수학적 표현식(equations)**과 **그림(Figure 2)**을 통해  
  **구현 가능하도록 상세히 설명**되어 있습니다.

---

### **Main Idea**  

- 기존의 **직접 매핑 방식** 대신,  
  신경망이 **잔차 함수 \(F(x) = H(x) - x\)**를 학습하도록 설계했습니다.  
- 이를 통해 **정체 매핑(identity mapping)**을 구현할 때,  
  단순히 **잔차 함수를 0으로 수렴시키는 방법**으로 **학습 난이도**를 줄였습니다.

---

### **Contribution**  

- 잔차 블록(Residual Block)을 사용해  
  깊은 네트워크에서도 **기울기 소실 문제**를 해결했습니다.  
- **지름길 연결(Shortcut Connection)**을 통해  
  **추가적인 매개변수나 계산 비용을 증가시키지 않으면서**  
  네트워크의 깊이를 자유롭게 늘릴 수 있게 했습니다.

---

## **4. 실험 및 결과**

### 💡 **제안 방법 검증 실험**  

- 실험을 통해 **잔차 학습**이 기존 방법의 한계를 어떻게 해결했는지 보여줍니다.  

---

### **Dataset**  

- **ImageNet**: 1000개 클래스, 약 120만 개의 학습 이미지  
- **CIFAR-10**: 10개 클래스, 6만 개의 작은 이미지  

---

### **Baseline**  

- **VGGNet**, **GoogLeNet**,  
- 동일한 깊이의 **일반 네트워크(Plain Networks)**와 비교  

---

### **결과**  

- **ResNet-152 모델**은 **ImageNet 대회**에서 **Top-5 오류율 3.57%**를 기록하며 **1위**를 차지했습니다.  
- **CIFAR-10**에서도 **잔차 네트워크**는 깊이가 증가해도 **성능이 안정적**이었지만,  
  동일한 깊이의 **일반 네트워크**는 **학습 오류가 증가**했습니다.

---

## **5. 결론 (배운 점)**

### 💡 **연구의 의의 및 한계**  

- **의의:**  
  - **잔차 학습(Residual Learning)**을 통해  
    깊은 신경망에서도 **학습 가능성(Trainability)**을 높였습니다.  

- **한계:**  
  - 논문에서는 **매우 깊은 네트워크의 구현 가능성**을 보여주었지만,  
  - 실제 응용에서는 **메모리 사용량**과 **계산 비용**이 문제가 될 수 있습니다.

---

### 💡 **배운 점 및 아쉬운 점**  

- **좋았던 점:**  
  - **다양한 데이터셋(ImageNet, CIFAR-10)**을 활용하여  
    방법론의 **일반화 성능(Generalization Performance)**을 보여주었습니다.

- **아쉬웠던 점:**  
  - **추가적인 메모리 사용량(memory usage)**에 대한 논의가 부족했습니다.  
  - **학습 속도 및 실제 응용에서의 최적화 방법**에 대한 설명이 더 있었으면 좋았을 것입니다.
