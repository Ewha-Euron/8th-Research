{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "outputId": "fe49658b-281c-4807-e241-7a3570c860ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Cloning into 'PPO-PyTorch'...\n",
            "remote: Enumerating objects: 368, done.\u001b[K\n",
            "remote: Counting objects: 100% (104/104), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 368 (delta 91), reused 68 (delta 68), pack-reused 264 (from 2)\u001b[K\n",
            "Receiving objects: 100% (368/368), 12.39 MiB | 17.04 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs\n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mrn6rpJpF8Sc",
        "outputId": "91e4d49e-467f-4d65-8195-a7eb48135ebd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "'./PPO-PyTorch/LICENSE' -> './LICENSE'\n",
            "'./PPO-PyTorch/make_gif.py' -> './make_gif.py'\n",
            "'./PPO-PyTorch/plot_graph.py' -> './plot_graph.py'\n",
            "'./PPO-PyTorch/PPO_colab.ipynb' -> './PPO_colab.ipynb'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png' -> './PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png' -> './PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png' -> './PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png' -> './PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png' -> './PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif' -> './PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif' -> './PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif' -> './PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif' -> './PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth' -> './PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth' -> './PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth' -> './PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/README.md' -> './PPO_preTrained/README.md'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth' -> './PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO.py' -> './PPO.py'\n",
            "'./PPO-PyTorch/README.md' -> './README.md'\n",
            "'./PPO-PyTorch/test.py' -> './test.py'\n",
            "'./PPO-PyTorch/train.py' -> './train.py'\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs\n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-7AbGA2F8Ut",
        "outputId": "fb0f3916-ad36-4efd-947e-959a687f8ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rbpSQTflGlAr",
        "outputId": "d6ca0173-130a-4303-92f7-9ab6643c207c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install swig\n",
        "\n",
        "# !pip install roboschool==1.0.7 gym==0.15.4\n",
        "\n",
        "# !pip install box2d-py\n",
        "\n",
        "# !pip install Box2D\n",
        "\n",
        "# !pip install pybullet\n",
        "\n",
        "# !pip install gym[box2d]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts :\n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT6VUBg-F8Zm",
        "outputId": "c0a31b71-78d3-456a-b46f-a191146573fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pybullet==3.2.5 in /usr/local/lib/python3.11/dist-packages (3.2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "\n",
        "# import roboschool\n",
        "!pip install pybullet==3.2.5 # 물리 시뮬레이션 환경\n",
        "\n",
        "import pybullet_envs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()):\n",
        "    device = torch.device('cuda:0')\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer: # 경험 데이터를 임시로 저장함 -> 일정 시간 모은 데이터로 advantage, return 등 계산 & 업데이트\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = [] # 로그 확률\n",
        "        self.rewards = []\n",
        "        self.state_values = [] # 다음 상태\n",
        "        self.is_terminals = [] # 종료 여부\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.state_values[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module): # Actor-Critic 구조 -> 정책, 가치 함수 처리\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor(정책) : 상태를 받아서 행동 분포 출력\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        # critic(가치 함수) # 상태를 받아서 가치 출력\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "\n",
        "    def set_action_std(self, new_action_std): # 정책 분포가 정규분포일 때 표준편차값 조정\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else: # Discrete 경우 필요 X\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError # act랑 evaluate 따로 사용해서 forward 사용 X\n",
        "\n",
        "\n",
        "    def act(self, state): # 행동 선택\n",
        "\n",
        "        # 연속 행동공간\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        # 이산 행동공간\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        # 연속, 이산 모두 적용\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action) # 로그 확률\n",
        "        state_val = self.critic(state) # 상태 가치\n",
        "\n",
        "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
        "\n",
        "\n",
        "    def evaluate(self, state, action): # PPO 손실 계산 시 필요한 반환값들\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "\n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "\n",
        "        return action_logprobs, state_values, dist_entropy"
      ],
      "metadata": {
        "id": "bHhK7PkVfNff",
        "outputId": "c8d7fb9b-3f79-4686-d998-a152cbb57fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Device set to : Tesla T4\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO 전체 구조\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "\n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ]) # Actor이랑 Critic 따로 학습률로 최적화\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std): # 표준 편차값 변경\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std): # 일정 주기마다 표준편차 감소 -> 탐험에서 활용으로\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state): # 현재 상태값 받아서 행동 결정\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob, state_val = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "            self.buffer.state_values.append(state_val)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self): ### 짱 중요한 부분\n",
        "\n",
        "        # Monte Carlo estimate of returns : 리턴값 계산 부분\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)): # reversed -> 뒤에서부터 역순으로 계산\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "\n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n",
        "\n",
        "        # calculate advantages\n",
        "        advantages = rewards.detach() - old_state_values.detach()\n",
        "\n",
        "\n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "\n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # 손실 계산\n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # PPO의 핵심 부분\n",
        "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
        "\n",
        "            # 최적화 & 정책 업데이트\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # 버퍼 초기화 -> 새 에피소드 준비\n",
        "        self.buffer.clear()\n",
        "\n",
        "\n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "\n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
      ],
      "metadata": {
        "id": "bbaExKGtfBFF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY1-DzVCF8eh",
        "outputId": "aa35ba58-1043-4553-f791-6006516aff6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "training environment name : CartPole-v1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run\n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zo4n6Hwo4-q",
        "outputId": "360c8177-791b-42dc-f1ef-18b3e97ce006"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "current logging run number for CartPole-v1 :  5\n",
            "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_5.csv\n",
            "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1gLSxyyo5-v",
        "outputId": "88ef3b44-1499-4df9-e51f-3af996eae6f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bool8이 not exist in the version of numpy being used -> 다운그레이드 과정 진\n",
        "!pip install numpy==1.26.0 #Downgrade to a compatible numpy version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPFvqnA_pXl6",
        "outputId": "e5166dc4-a48e-4bb5-8f55-5db185851c5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.0\n",
            "  Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Using cached numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO 전체 학습 루프\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# 학습 루프 시작 부분\n",
        "while time_step <= max_training_timesteps:\n",
        "\n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "\n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state) # 행동 선택\n",
        "        state, reward, done, _ = env.step(action) # 환경 실행\n",
        "\n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward) # 보상 저장\n",
        "        ppo_agent.buffer.is_terminals.append(done) # 종료했는지 저장\n",
        "\n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0: ## 연속형만 표준편차 감소\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "\n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward # 에피소드별로 누적 보상 업데이트\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWKiU6M1o6SE",
        "outputId": "1e6f983a-3788-475c-a237-61ebff176d98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Started training at (GMT) :  2025-05-08 02:36:27\n",
            "============================================================================================\n",
            "Episode : 71 \t\t Timestep : 1600 \t\t Average Reward : 22.46\n",
            "Episode : 131 \t\t Timestep : 3200 \t\t Average Reward : 26.08\n",
            "Episode : 178 \t\t Timestep : 4800 \t\t Average Reward : 34.55\n",
            "Episode : 209 \t\t Timestep : 6400 \t\t Average Reward : 51.19\n",
            "Episode : 229 \t\t Timestep : 8000 \t\t Average Reward : 79.25\n",
            "Episode : 247 \t\t Timestep : 9600 \t\t Average Reward : 86.67\n",
            "Episode : 259 \t\t Timestep : 11200 \t\t Average Reward : 140.17\n",
            "Episode : 274 \t\t Timestep : 12800 \t\t Average Reward : 105.73\n",
            "Episode : 285 \t\t Timestep : 14400 \t\t Average Reward : 142.27\n",
            "Episode : 296 \t\t Timestep : 16000 \t\t Average Reward : 138.36\n",
            "Episode : 307 \t\t Timestep : 17600 \t\t Average Reward : 148.36\n",
            "Episode : 316 \t\t Timestep : 19200 \t\t Average Reward : 182.89\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:39\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 322 \t\t Timestep : 20800 \t\t Average Reward : 231.83\n",
            "Episode : 327 \t\t Timestep : 22400 \t\t Average Reward : 333.6\n",
            "Episode : 337 \t\t Timestep : 24000 \t\t Average Reward : 167.2\n",
            "Episode : 347 \t\t Timestep : 25600 \t\t Average Reward : 170.5\n",
            "Episode : 355 \t\t Timestep : 27200 \t\t Average Reward : 173.0\n",
            "Episode : 362 \t\t Timestep : 28800 \t\t Average Reward : 242.71\n",
            "Episode : 367 \t\t Timestep : 30400 \t\t Average Reward : 306.6\n",
            "Episode : 373 \t\t Timestep : 32000 \t\t Average Reward : 262.17\n",
            "Episode : 380 \t\t Timestep : 33600 \t\t Average Reward : 215.14\n",
            "Episode : 386 \t\t Timestep : 35200 \t\t Average Reward : 305.0\n",
            "Episode : 393 \t\t Timestep : 36800 \t\t Average Reward : 227.0\n",
            "Episode : 398 \t\t Timestep : 38400 \t\t Average Reward : 320.8\n",
            "Episode : 403 \t\t Timestep : 40000 \t\t Average Reward : 291.8\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:10\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 408 \t\t Timestep : 41600 \t\t Average Reward : 344.4\n",
            "Episode : 413 \t\t Timestep : 43200 \t\t Average Reward : 322.8\n",
            "Episode : 417 \t\t Timestep : 44800 \t\t Average Reward : 375.0\n",
            "Episode : 422 \t\t Timestep : 46400 \t\t Average Reward : 292.4\n",
            "Episode : 427 \t\t Timestep : 48000 \t\t Average Reward : 325.4\n",
            "Episode : 431 \t\t Timestep : 49600 \t\t Average Reward : 400.0\n",
            "Episode : 436 \t\t Timestep : 51200 \t\t Average Reward : 320.8\n",
            "Episode : 440 \t\t Timestep : 52800 \t\t Average Reward : 385.0\n",
            "Episode : 445 \t\t Timestep : 54400 \t\t Average Reward : 386.2\n",
            "Episode : 451 \t\t Timestep : 56000 \t\t Average Reward : 256.83\n",
            "Episode : 455 \t\t Timestep : 57600 \t\t Average Reward : 359.75\n",
            "Episode : 461 \t\t Timestep : 59200 \t\t Average Reward : 281.33\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:01:45\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 465 \t\t Timestep : 60800 \t\t Average Reward : 347.5\n",
            "Episode : 472 \t\t Timestep : 62400 \t\t Average Reward : 265.0\n",
            "Episode : 478 \t\t Timestep : 64000 \t\t Average Reward : 246.5\n",
            "Episode : 483 \t\t Timestep : 65600 \t\t Average Reward : 309.4\n",
            "Episode : 488 \t\t Timestep : 67200 \t\t Average Reward : 374.8\n",
            "Episode : 492 \t\t Timestep : 68800 \t\t Average Reward : 400.0\n",
            "Episode : 496 \t\t Timestep : 70400 \t\t Average Reward : 400.0\n",
            "Episode : 504 \t\t Timestep : 72000 \t\t Average Reward : 188.12\n",
            "Episode : 514 \t\t Timestep : 73600 \t\t Average Reward : 169.1\n",
            "Episode : 519 \t\t Timestep : 75200 \t\t Average Reward : 301.0\n",
            "Episode : 524 \t\t Timestep : 76800 \t\t Average Reward : 308.8\n",
            "Episode : 528 \t\t Timestep : 78400 \t\t Average Reward : 382.0\n",
            "Episode : 532 \t\t Timestep : 80000 \t\t Average Reward : 400.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:14\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 536 \t\t Timestep : 81600 \t\t Average Reward : 369.75\n",
            "Episode : 540 \t\t Timestep : 83200 \t\t Average Reward : 400.0\n",
            "Episode : 544 \t\t Timestep : 84800 \t\t Average Reward : 400.0\n",
            "Episode : 549 \t\t Timestep : 86400 \t\t Average Reward : 393.2\n",
            "Episode : 553 \t\t Timestep : 88000 \t\t Average Reward : 356.5\n",
            "Episode : 557 \t\t Timestep : 89600 \t\t Average Reward : 382.25\n",
            "Episode : 561 \t\t Timestep : 91200 \t\t Average Reward : 363.5\n",
            "Episode : 566 \t\t Timestep : 92800 \t\t Average Reward : 350.2\n",
            "Episode : 570 \t\t Timestep : 94400 \t\t Average Reward : 381.75\n",
            "Episode : 575 \t\t Timestep : 96000 \t\t Average Reward : 357.4\n",
            "Episode : 581 \t\t Timestep : 97600 \t\t Average Reward : 263.67\n",
            "Episode : 588 \t\t Timestep : 99200 \t\t Average Reward : 229.57\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:02:45\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2025-05-08 02:36:27\n",
            "Finished training at (GMT) :  2025-05-08 02:39:12\n",
            "Total training time  :  0:02:45\n",
            "============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "outputs": [],
      "source": [
        "################################ End of Part II ################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SZWyhkq9Gxm5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 970
        },
        "outputId": "476af3ad-8658-4a3f-83b1-adeb6fd62c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.0\n",
            "    Uninstalling numpy-1.26.0:\n",
            "      Successfully uninstalled numpy-1.26.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "b68552bd313a48bd97f2e899752f8e22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 257.0\n",
            "Episode: 2 \t\t Reward: 310.0\n",
            "Episode: 3 \t\t Reward: 231.0\n",
            "Episode: 4 \t\t Reward: 340.0\n",
            "Episode: 5 \t\t Reward: 388.0\n",
            "Episode: 6 \t\t Reward: 297.0\n",
            "Episode: 7 \t\t Reward: 251.0\n",
            "Episode: 8 \t\t Reward: 276.0\n",
            "Episode: 9 \t\t Reward: 272.0\n",
            "Episode: 10 \t\t Reward: 400.0\n",
            "============================================================================================\n",
            "average test reward : 302.2\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "# PPO 학습이 끝난 후 테스트 단계\n",
        "################## hyperparameters ##################\n",
        "\n",
        "!pip install numpy==1.23.5\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "# Test 루프 실행 부분\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bY-E5HGcGxiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "1d1f7f3d-8950-4ad4-c23c-244d0aa613b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_1.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_2.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_3.csv\n",
            "data shape :  (0, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_4.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_5.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-5ab7c47e11ef>:77: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df_concat = pd.concat(all_runs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
            "============================================================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# 시각화 설정(smoothing, 색상 배열)\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "\n",
        "    print(\"data shape : \", data.shape)\n",
        "\n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg: # 평균 그래프\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean() # 부드러운 선\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean() # 변동성 큰 선\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else: # 개별 그래프(각 실행별로)\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally.\n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "outputs": [],
      "source": [
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839544ef-7813-47c1-a4dd-9de7ca3810d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roboschool==1.0.7\n",
            "  Using cached roboschool-1.0.7.tar.gz (12.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym==0.15.4\n",
            "  Using cached gym-0.15.4-py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.15.2)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (1.17.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0 (from gym==0.15.4)\n",
            "  Using cached pyglet-1.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting cloudpickle~=1.2.0 (from gym==0.15.4)\n",
            "  Using cached cloudpickle-1.2.2-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from gym==0.15.4) (4.11.0.86)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (1.0.0)\n",
            "Using cached cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Using cached pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
            "Building wheels for collected packages: roboschool\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for roboschool (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for roboschool\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for roboschool\n",
            "Failed to build roboschool\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (roboschool)\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gym[classic_control] in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Using cached pygame-2.1.0.tar.gz (5.8 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1 \t\t Reward: 297.0\n",
            "============================================================================================\n",
            "total number of frames / timesteps / images saved :  297\n",
            "average test reward : 297.0\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "############################# save images for gif ##############################\n",
        "# PPO 모델이 Test되는 모습을 프레임 단위로 저장\n",
        "\n",
        "!pip install roboschool==1.0.7 gym==0.15.4 # This line should install the necessary module. If there is any conflict, try with a different gym version.\n",
        "!pip install gym[classic_control] # Install classic control environment\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import gym\n",
        "\n",
        "# import roboschool # this import can be removed if you are using environments from gym[classic_control] such as CartPole-v1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name;\n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "# 에피소드 실행, 이미지 저장\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer\n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BoVshl_ZHK7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "858d8d01-96c6-44c6-9f4a-3732be222d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "total frames in gif :  30\n",
            "total duration of gif : 4.5 seconds\n",
            "saved gif at :  PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "######################## generate gif from saved images ########################\n",
        "# 저장해둔 이미지들을 불러와서 GIF로 변환\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir)) # sorted로 숫자순으로 정렬\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# GIF 생성 부분\n",
        "img, *imgs = [Image.open(f) for f in img_paths] # 1st 이미지 기준으로 GIF 생성 + 나머지 이미지 이어 붙임\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "print(\"============================================================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "20d1bR8xHK5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a362b1c-9370-48f2-a03a-2a3f0d579f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================================================\n",
            "PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif\t\t0.14 MB\n",
            "============================================================================================\n"
          ]
        }
      ],
      "source": [
        "############################# check gif byte size ##############################\n",
        "# 생성된 GIF 파일 크기 확인\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "# 파일 크기 확인하고 출력\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path) # 파일 크기 가져와서 file_size에 할당\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "outputs": [],
      "source": [
        "################################# End of Part V ################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}