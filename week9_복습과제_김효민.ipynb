{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bTa3m2jMw_ZQ"
      },
      "outputs": [],
      "source": [
        "# model.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, attn_pdrop):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        # |q| : (batch_size, n_heads, q_len, d_k)\n",
        "        # |k| : (batch_size, n_heads, k_len, d_k)\n",
        "        # |v| : (batch_size, n_heads, v_len, d_v)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
        "        attn_score.masked_fill_(attn_mask, -1e9)\n",
        "        # |attn_scroe| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "        # |output| : (batch_size, n_heads, q_len, d_v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_pdrop):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "\n",
        "        self.WQ = nn.Linear(d_model, d_model)\n",
        "        self.WK = nn.Linear(d_model, d_model)\n",
        "        self.WV = nn.Linear(d_model, d_model)\n",
        "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_pdrop)\n",
        "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # |Q| : (batch_size, q_len(=seq_len), d_model)\n",
        "        # |K| : (batch_size, k_len(=seq_len), d_model)\n",
        "        # |V| : (batch_size, v_len(=seq_len), d_model)\n",
        "        # |attn_mask| : (batch_size, q_len, k_len)\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "        # |q_heads| : (batch_size, n_heads, q_len, d_k), |k_heads| : (batch_size, n_heads, k_len, d_k), |v_heads| : (batch_size, n_heads, v_len, d_v)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        # |attn_mask| : (batch_size, n_heads, q_len, k_len)\n",
        "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "        # |attn| : (batch_size, n_heads, q_len, d_v)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len, k_len)\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        # |attn| : (batch_size, q_len, n_heads * d_v)\n",
        "        outputs = self.linear(attn)\n",
        "        # |outputs| : (batch_size, q_len, d_model)\n",
        "\n",
        "        return outputs, attn_weights\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        outputs = self.gelu(self.linear1(inputs))\n",
        "        # |outputs| : (batch_size, seq_len, d_ff)\n",
        "        outputs = self.linear2(outputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, attn_pdrop, resid_pdrop):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, attn_pdrop)\n",
        "        self.dropout1 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, inputs, attn_mask):\n",
        "        # |inputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "        attn_outputs = self.dropout1(attn_outputs)\n",
        "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "        # |attn_outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attn_weights| : (batch_size, n_heads, q_len(=seq_len), k_len(=seq_len))\n",
        "\n",
        "        ffn_outputs = self.ffn(attn_outputs)\n",
        "        ffn_outputs = self.dropout2(ffn_outputs)\n",
        "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
        "        # |ffn_outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        return ffn_outputs, attn_weights\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_pdrop, attn_pdrop, resid_pdrop, pad_id):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.pad_id = pad_id\n",
        "\n",
        "        # layers\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_pdrop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_pdrop, resid_pdrop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
        "        position_pad_mask = inputs.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "        # |positions| : (batch_size, seq_len)\n",
        "\n",
        "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "\n",
        "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "        # |attn_pad_mask| : (batch_size, seq_len, seq_len)\n",
        "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "        # |subsequent_mask| : (batch_size, seq_len, seq_len)\n",
        "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
        "        # |attn_mask| : (batch_size, seq_len, seq_len)\n",
        "\n",
        "        attention_weights = []\n",
        "        for layer in self.layers:\n",
        "            outputs, attn_weights = layer(outputs, attn_mask)\n",
        "            # |outputs| : (batch_size, seq_len, d_model)\n",
        "            # |attn_weights| : (batch_size, n_heads, seq_len, seq_len)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def get_attention_padding_mask(self, q, k, pad_id):\n",
        "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "        # |attn_pad_mask| : (batch_size, q_len, k_len)\n",
        "\n",
        "        return attn_pad_mask\n",
        "\n",
        "    def get_attention_subsequent_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "        # |subsequent_mask| : (batch_size, q_len, q_len)\n",
        "\n",
        "        return subsequent_mask\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len=512,\n",
        "                 d_model=768,\n",
        "                 n_layers=12,\n",
        "                 n_heads=12,\n",
        "                 d_ff=3072,\n",
        "                 embd_pdrop=0.1,\n",
        "                 attn_pdrop=0.1,\n",
        "                 resid_pdrop=0.1,\n",
        "                 pad_id=0):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads, d_ff,\n",
        "                                          embd_pdrop, attn_pdrop, resid_pdrop, pad_id)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.decoder(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super(GPTLMHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear(outputs)\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_pdrop=0.1):\n",
        "        super(GPTClsHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "        # Classification\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_pdrop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # |inputs| : (batch_size, seq_len)\n",
        "\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "        # |outputs| : (batch_size, seq_len, d_model)\n",
        "        # |attention_weights| : [(batch_size, n_heads, seq_len, seq_len)] * n_layers\n",
        "\n",
        "        lm_logits = self.linear1(outputs)\n",
        "        # |lm_logits| : (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
        "        # |outputs| : (batch_size, d_model)\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "        # |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "        return lm_logits, cls_logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prenlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpnjzFg8x-RO",
        "outputId": "42d14d64-a277-4451-d34a-169e753339a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prenlp\n",
            "  Downloading prenlp-0.0.13-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting nltk==3.2.5 (from prenlp)\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konlpy (from prenlp)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from prenlp) (0.2.0)\n",
            "Collecting ijson (from prenlp)\n",
            "  Downloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting py7zr==0.5b5 (from prenlp)\n",
            "  Downloading py7zr-0.5b5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from nltk==3.2.5->prenlp) (1.17.0)\n",
            "Collecting texttable (from py7zr==0.5b5->prenlp)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->prenlp)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy->prenlp) (24.2)\n",
            "Downloading prenlp-0.0.13-py3-none-any.whl (30 kB)\n",
            "Downloading py7zr-0.5b5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392147 sha256=7d26b55b2b9c648d48fc70a450c83f986cc3ba0beab8f7185525e4bab5c71c30\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/69/e3/8b11e6490c8f20fcab5f6a3321d60fcc0b26ed6f7745ad95b4\n",
            "Successfully built nltk\n",
            "Installing collected packages: texttable, py7zr, nltk, JPype1, ijson, konlpy, prenlp\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JPype1-1.5.2 ijson-3.4.0 konlpy-0.6.0 nltk-3.2.5 prenlp-0.0.13 py7zr-0.5b5 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization.py\n",
        "\n",
        "\"\"\"Code from\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/translation-transformer/tokenization.py\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/text-classification-transformer/tokenization.py\n",
        "\"\"\"\n",
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.sep_token = sep_token\n",
        "        self.cls_token = cls_token\n",
        "        self.mask_token = mask_token\n",
        "        self.vocab = OrderedDict()\n",
        "        self.ids_to_tokens = OrderedDict()\n",
        "\n",
        "        # Build vocab and ids_to_tokens\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
        "            for i, line in enumerate(reader.readlines()):\n",
        "                token = line.split()[0]\n",
        "                self.vocab[token] = i\n",
        "        for token, id in self.vocab.items():\n",
        "            self.ids_to_tokens[id] = token\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Tokenize given text.\n",
        "        \"\"\"\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def convert_token_to_id(self, token: str) -> int:\n",
        "        \"\"\"Convert a token (str) in an id (integer) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def convert_id_to_token(self, id: int) -> str:\n",
        "        \"\"\"Convert an id (integer) in a token (str) using the vocab.\n",
        "        \"\"\"\n",
        "        return self.ids_to_tokens.get(id, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "        \"\"\"Convert list of tokens in list of ids using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
        "        \"\"\"Convert list of ids in list of tokens using the vocab.\n",
        "        \"\"\"\n",
        "        return [self.convert_id_to_token(id) for id in ids]\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"Vocabulary size.\n",
        "        \"\"\"\n",
        "        return len(self.vocab)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        \"\"\"Id of pad_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.pad_token)\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self) -> int:\n",
        "        \"\"\"Id of unk_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        \"\"\"Id of bos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.bos_token)\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        \"\"\"Id of eos_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.eos_token)\n",
        "\n",
        "    @property\n",
        "    def sep_token_id(self) -> int:\n",
        "        \"\"\"Id of sep_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.sep_token)\n",
        "\n",
        "    @property\n",
        "    def cls_token_id(self) -> int:\n",
        "        \"\"\"Id of cls_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.cls_token)\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self) -> int:\n",
        "        \"\"\"Id of mask_token in the vocab.\n",
        "        \"\"\"\n",
        "        return self.convert_token_to_id(self.mask_token)\n",
        "\n",
        "class PretrainedTokenizer(Tokenizer):\n",
        "    def __init__(self, pretrained_model: str, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        tokenizer = SentencePiece.load(pretrained_model)\n",
        "\n",
        "        super(PretrainedTokenizer, self).__init__(tokenizer, vocab_file, pad_token, unk_token, bos_token, eos_token)\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        \"\"\"Detokenize given tokens.\n",
        "        \"\"\"\n",
        "        return self.tokenizer.detokenize(tokens)"
      ],
      "metadata": {
        "id": "nnvd2pPhxi4q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WiZlPcxvyRnl",
        "outputId": "c1a54cd9-f4f0-4a56-b90e-4b4d0230b825"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch-optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch-optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->torch-optimizer)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch-optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-ranger, torch-optimizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-ranger-0.1.1 torch-optimizer-0.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "357f9923a7f1456a8230183358328149"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.py\n",
        "\n",
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch_optimizer as optim\n",
        "\n",
        "\n",
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        _args = args[0].args\n",
        "\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "\n",
        "        if _args.distributed:\n",
        "            if _args.local_rank == 0:\n",
        "                print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "        else:\n",
        "            print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args, train_loader, test_loader, tokenizer):\n",
        "        self.args = args\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.pad_id = tokenizer.pad_token_id\n",
        "        self.eos_id = tokenizer.eos_token_id\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu', args.local_rank)\n",
        "        self.writer = SummaryWriter() if args.local_rank in [-1, 0] else None\n",
        "        self.n_gpus = torch.distributed.get_world_size() if args.distributed else torch.cuda.device_count()\n",
        "        assert args.pretrain != args.finetune # Do not set both finetune and pretrain arguments to the same (True, False)\n",
        "\n",
        "        if args.pretrained_model:\n",
        "            self.gpt = torch.load(args.pretrained_model)\n",
        "        else:\n",
        "            self.gpt = GPT(vocab_size=self.vocab_size,\n",
        "                           seq_len=args.max_seq_len,\n",
        "                           d_model=args.hidden,\n",
        "                           n_layers=args.n_layers,\n",
        "                           n_heads=args.n_attn_heads,\n",
        "                           d_ff=args.ffn_hidden,\n",
        "                           embd_pdrop=args.embd_dropout,\n",
        "                           attn_pdrop=args.attn_dropout,\n",
        "                           resid_pdrop=args.resid_dropout,\n",
        "                           pad_id=self.pad_id)\n",
        "\n",
        "        if args.pretrain:\n",
        "            self.model = GPTLMHead(self.gpt)\n",
        "            self.model.to(self.device)\n",
        "        if args.finetune:\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "            self.model = GPTClsHead(self.gpt, n_class=len(label_dict), cls_token_id=self.eos_id)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        if args.distributed:\n",
        "            self.model = DistributedDataParallel(self.model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
        "\n",
        "        self.optimizer = optim.RAdam(self.model.parameters(), args.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index = self.pad_id).to(self.device)\n",
        "        self.cls_criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    @timeit\n",
        "    def train(self, epoch):\n",
        "        if self.args.pretrain:\n",
        "            self.pretrain(epoch)\n",
        "        if self.args.finetune:\n",
        "            self.finetune(epoch)\n",
        "\n",
        "    def pretrain(self, epoch):\n",
        "        losses = 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs = batch[0].to(self.device)\n",
        "            targets = inputs[:, 1:].contiguous()\n",
        "            # |inputs| : (batch_size, seq_len), |targets| : (batch_size, seq_len-1)\n",
        "\n",
        "            lm_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size)\n",
        "\n",
        "            loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "            losses += loss.item()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/pre-train', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f}'.format(i, i, n_batches, losses/i))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f}'.format(epoch, self.args.local_rank, losses/n_batches))\n",
        "\n",
        "    def finetune(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset) # n_batches = batch size per GPU\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "            # |inputs| : (batch_size, seq_len), |labels| : (batch_size)\n",
        "\n",
        "            lm_logits, cls_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            # |lm_logits| : (batch_size, seq_len-1, vocab_size), |cls_logits| : (batch_size, n_class)\n",
        "\n",
        "            lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "            cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "            loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "            losses += loss.item()\n",
        "            acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "            accs += acc\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/fine-tune', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                self.writer.add_scalar('Accuracy/fine-tune', acc, ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f} Acc: {:.1f}%'.format(i, i, n_batches, losses/i, accs/i*100.))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.test_loader), len(self.test_loader.dataset)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.test_loader):\n",
        "                if self.args.pretrain:\n",
        "                    inputs = batch.to(self.device)\n",
        "                    targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "                    lm_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "                    losses += loss.item()\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/pre-train(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "\n",
        "                elif self.args.finetune:\n",
        "                    inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "\n",
        "                    lm_logits, cls_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "                    cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "                    loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "                    losses += loss.item()\n",
        "                    acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "                    accs += acc\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/fine-tune(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                        self.writer.add_scalar('Accuracy/fine-tune(eval)', acc, ((epoch-1)*n_batches)+i)\n",
        "\n",
        "        print('Eval Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def save(self, epoch, model_prefix='model', root='.model'):\n",
        "        path = Path(root) / (model_prefix + '.ep%d' % epoch)\n",
        "        if not path.parent.exists():\n",
        "            path.parent.mkdir()\n",
        "\n",
        "        if self.args.distributed:\n",
        "            if self.args.local_rank == 0:\n",
        "                torch.save(self.gpt, path)\n",
        "        else:\n",
        "            torch.save(self.gpt, path)"
      ],
      "metadata": {
        "id": "TFyt4IQ_yL5l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S0A_HJR1ASA",
        "outputId": "127f5ef8-935c-42a0-e1cb-fadc3d2b56be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"localhost\" --master_port=1112 main.py --train_corpus .data/wikitext-103/wiki.train --vocab_file wiki103.vocab --pretrained_sp_model wiki103.model --pretrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpCBv3BN1eU8",
        "outputId": "d499c4ec-7bd4-4c29-e253-da1381534cf6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  main()\n",
            "W0512 14:11:27.372000 5132 torch/distributed/run.py:792] \n",
            "W0512 14:11:27.372000 5132 torch/distributed/run.py:792] *****************************************\n",
            "W0512 14:11:27.372000 5132 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0512 14:11:27.372000 5132 torch/distributed/run.py:792] *****************************************\n",
            "/usr/bin/python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n",
            "/usr/bin/python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n",
            "E0512 14:11:27.626000 5132 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 5150) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typing_extensions.py\", line 3253, in wrapper\n",
            "    return arg(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 204, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 189, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2025-05-12_14:11:27\n",
            "  host      : ef2560b61db3\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 2 (pid: 5151)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-05-12_14:11:27\n",
            "  host      : ef2560b61db3\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 5150)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.launch --nproc_per_node=2 --nnodes=1 --node_rank=0 --master_addr=\"localhost\" --master_port=1112 main.py --train_corpus .data/aclImdb/imdb.train --test_corpus .data/aclImdb/imdb.test --vocab_file wiki103.vocab --pretrained_sp_model wiki103.model --pretrained_model .model/model.ep22 --finetune --do_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv_xg0ul1i__",
        "outputId": "689396e8-9fc9-41e3-91e8-c110c3a7ffb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\n",
            "and will be removed in future. Use torchrun.\n",
            "Note that --use-env is set by default in torchrun.\n",
            "If your script expects `--local-rank` argument to be set, please\n",
            "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
            "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
            "further instructions\n",
            "\n",
            "  main()\n",
            "W0512 14:11:43.462000 5214 torch/distributed/run.py:792] \n",
            "W0512 14:11:43.462000 5214 torch/distributed/run.py:792] *****************************************\n",
            "W0512 14:11:43.462000 5214 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "W0512 14:11:43.462000 5214 torch/distributed/run.py:792] *****************************************\n",
            "/usr/bin/python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n",
            "/usr/bin/python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n",
            "E0512 14:11:43.571000 5214 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 2) local_rank: 0 (pid: 5224) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/typing_extensions.py\", line 3253, in wrapper\n",
            "    return arg(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 204, in main\n",
            "    launch(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py\", line 189, in launch\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2025-05-12_14:11:43\n",
            "  host      : ef2560b61db3\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 2 (pid: 5225)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-05-12_14:11:43\n",
            "  host      : ef2560b61db3\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 2 (pid: 5224)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorboard --logdir=runs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T26WMwd11mei",
        "outputId": "6fb8806b-0457-4066-ae54-d90fe8b112be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-05-12 14:19:33.507988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747059573.544313    7244 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747059573.554689    7244 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data_utils.py\n",
        "\n",
        "from typing import Iterable, Union, List\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "class PretrainInputExample:\n",
        "    \"\"\"A single example for unsupervised pre-training.\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "\n",
        "class ClsInputExample:\n",
        "    \"\"\"A single example for supervised fine-tuning (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, text: str, label: str):\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "class PretrainInputFeatures:\n",
        "    \"\"\"A single set of features of pre-training data.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int]):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "class ClsInputFeatures:\n",
        "    \"\"\"A single set of features of fine-tuning data (classification).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_ids: List[int], label_id: int):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 tokenizer,\n",
        "                                 args,\n",
        "                                 mode):\n",
        "    bos_token = tokenizer.bos_token\n",
        "    eos_token = tokenizer.eos_token\n",
        "    pad_token = tokenizer.pad_token\n",
        "\n",
        "    # Build label dict(vocab) with examples\n",
        "    if args.finetune:\n",
        "        if mode == 'train':\n",
        "            labels = sorted(list(set([example.label for example in examples])))\n",
        "            label_dict = {label: i for i, label in enumerate(labels)}\n",
        "            with open(args.cached_label_dict, 'w') as file:\n",
        "                json.dump(label_dict, file,  indent=4)\n",
        "        elif mode == 'test':\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "\n",
        "    # Create features\n",
        "    features = []\n",
        "    for i, example in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "        tokens = [bos_token] + tokens[:args.max_seq_len-2] + [eos_token] # BOS, EOS\n",
        "        tokens += [pad_token] * (args.max_seq_len - len(tokens))\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        if args.finetune:\n",
        "            label_id = label_dict.get(example.label)\n",
        "\n",
        "        if args.pretrain:\n",
        "            feature = PretrainInputFeatures(input_ids)\n",
        "        elif args.finetune:\n",
        "            feature = ClsInputFeatures(input_ids, label_id)\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_examples(args, tokenizer, mode='train'):\n",
        "    if args.local_rank not in [-1, 0]: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    assert mode in ('train', 'test')\n",
        "    cached_features_file = Path('cached_features_{}_{}_{}'.format('pretrain' if args.pretrain else 'finetune', mode, args.max_seq_len))\n",
        "\n",
        "    if cached_features_file.exists():\n",
        "        print('Loading features from cached file', cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        corpus_path = args.train_corpus if mode=='train' else args.test_corpus\n",
        "        with open(corpus_path, 'r', encoding='utf-8') as reader:\n",
        "            corpus = reader.readlines()\n",
        "\n",
        "        # Create examples\n",
        "        if args.pretrain:\n",
        "            corpus = list(map(lambda x: x.strip(), corpus))\n",
        "            corpus = list(filter(lambda x: len(x) > 0, corpus))\n",
        "            examples = [PretrainInputExample(text) for text in corpus]\n",
        "        elif args.finetune:\n",
        "            corpus = list(map(lambda x: x.split('\\t'), corpus))\n",
        "            corpus = list(map(lambda x: list(map(lambda y: y.strip(), x)), corpus))\n",
        "            corpus = list(map(lambda x: list(filter(lambda y: len(y) > 0, x)), corpus))\n",
        "            examples = [ClsInputExample(text, label) for label, text in corpus]\n",
        "\n",
        "        # Convert examples to features\n",
        "        features = convert_examples_to_features(examples, tokenizer, args, mode)\n",
        "\n",
        "        print('Saving features into cached file', cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0: # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "        dist.barrier()\n",
        "\n",
        "    # Create dataset with features\n",
        "    if args.pretrain:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids)\n",
        "    elif args.finetune:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([feature.label_id for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_label_ids)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "SasiTR3x2Q5G"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --train_corpus /content/data/wiki.train \\\n",
        "                --vocab_file /content/data/wiki.vocab \\\n",
        "                --pretrained_sp_model /content/data/wiki.model \\\n",
        "                --pretrain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU-8cg3P4DkI",
        "outputId": "91e0dcd1-8e9b-4f71-c7b0-36af7186579b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/main.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab.py\n",
        "\n",
        "\"\"\"Code from\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/translation-transformer/vocab.py\n",
        "    - https://github.com/lyeoni/nlp-tutorial/blob/master/text-classification-transformer/vocab.py\n",
        "\"\"\"\n",
        "import argparse\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "def build(args):\n",
        "    tokenizer = SentencePiece.train(input = args.corpus, model_prefix = args.prefix,\n",
        "                                    vocab_size = args.vocab_size,\n",
        "                                    model_type = args.model_type,\n",
        "                                    character_coverage = args.character_coverage,\n",
        "                                    max_sentence_length = args.max_sentence_length,\n",
        "                                    pad_token = args.pad_token,\n",
        "                                    unk_token = args.unk_token,\n",
        "                                    bos_token = args.bos_token,\n",
        "                                    eos_token = args.eos_token)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--corpus',      required=True,           type=str, help='one-sentence-per-line corpus file')\n",
        "    parser.add_argument('--prefix',      required=True,           type=str, help='output vocab(or sentencepiece model) name prefix')\n",
        "\n",
        "    parser.add_argument('--vocab_size',          default=16000,   type=int, help='the maximum size of the vocabulary')\n",
        "    parser.add_argument('--character_coverage',  default=1.0,     type=float,\n",
        "                        help='amount of characters covered by the model, good defaults are: 0.9995 for languages with rich character set\\\n",
        "                             like Japanse or Chinese and 1.0 for other languages with small character set')\n",
        "    parser.add_argument('--model_type',          default='bpe',   type=str, help='sentencepiece model type. Choose from unigram, bpe, char, or word')\n",
        "    parser.add_argument('--max_sentence_length', default=100000,  type=int, help='The maximum input sequence length')\n",
        "    parser.add_argument('--pad_token',           default='[PAD]', type=str, help='token that indicates padding')\n",
        "    parser.add_argument('--unk_token',           default='[UNK]', type=str, help='token that indicates unknown word')\n",
        "    parser.add_argument('--bos_token',           default='[BOS]', type=str, help='token that indicates beginning of sentence')\n",
        "    parser.add_argument('--eos_token',           default='[EOS]', type=str, help='token that indicates end of sentence')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    build(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "dFsH3TJHzdbq",
        "outputId": "63a81fb3-0c82-47e4-c459-84f3a0f68e3a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --corpus CORPUS --prefix PREFIX\n",
            "                                [--vocab_size VOCAB_SIZE]\n",
            "                                [--character_coverage CHARACTER_COVERAGE]\n",
            "                                [--model_type MODEL_TYPE]\n",
            "                                [--max_sentence_length MAX_SENTENCE_LENGTH]\n",
            "                                [--pad_token PAD_TOKEN]\n",
            "                                [--unk_token UNK_TOKEN]\n",
            "                                [--bos_token BOS_TOKEN]\n",
            "                                [--eos_token EOS_TOKEN]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --corpus, --prefix\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# main.py\n",
        "\n",
        "import argparse\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    print(args)\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.distributed:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        dist.init_process_group(backend='nccl')\n",
        "\n",
        "    # Load pretrained tokenizer\n",
        "    tokenizer = PretrainedTokenizer(pretrained_model=args.pretrained_sp_model, vocab_file=args.vocab_file)\n",
        "\n",
        "    # Build DataLoader\n",
        "    train_dataset = create_examples(args, tokenizer, mode='train')\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.batch_size, num_workers=args.n_workers)\n",
        "    if args.do_eval:\n",
        "        test_dataset = create_examples(args, tokenizer, mode='test')\n",
        "        test_sampler = RandomSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n",
        "        test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.batch_size, num_workers=args.n_workers)\n",
        "\n",
        "    # Build Trainer\n",
        "    trainer = Trainer(args=args,\n",
        "                      train_loader=train_loader,\n",
        "                      test_loader=test_loader if args.do_eval else None,\n",
        "                      tokenizer=tokenizer)\n",
        "\n",
        "    # Train\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        trainer.train(epoch)\n",
        "        trainer.save(epoch, args.output_model_prefix)\n",
        "        if args.do_eval:\n",
        "            trainer.evaluate(epoch)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--train_corpus',           required=True,     type=str, help='corpus for either pre-train or fine-tune')\n",
        "    parser.add_argument('--vocab_file',             required=True,     type=str, help='pretrained vocabulary')\n",
        "    parser.add_argument('--pretrained_sp_model',    required=True,     type=str, help='pretrained sentencepiece model')\n",
        "    parser.add_argument('--pretrain',               action='store_true')\n",
        "    parser.add_argument('--finetune',               action='store_true')\n",
        "    parser.add_argument('--do_eval',                action='store_true')\n",
        "\n",
        "    parser.add_argument('--test_corpus',            default=None,     type=str, help='corpus for either pre-train or fine-tune evaluation')\n",
        "    parser.add_argument('--pretrained_model',       default=None,     type=str, help='pretrained GPT model path')\n",
        "    parser.add_argument('--output_model_prefix',    default='model',  type=str, help='output model name prefix')\n",
        "    # Input parameters\n",
        "    parser.add_argument('--batch_size',     default=64,    type=int,   help='batch size')\n",
        "    parser.add_argument('--max_seq_len',    default=512,   type=int,   help='the maximum size of the input sequence')\n",
        "    parser.add_argument('--n_workers',      default=4,     type=int,   help='the number of workers')\n",
        "    # Train parameters\n",
        "    parser.add_argument('--epochs',         default=100,       type=int,   help='the number of epochs')\n",
        "    parser.add_argument('--lr',             default=1.5e-4,    type=float, help='initial learning rate')\n",
        "    parser.add_argument('--auxiliary_ratio',default=.25,       type=float, help='weight of auxiliary objective')\n",
        "    parser.add_argument('--local_rank',     default=-1,        type=int,   help='node rank for distributed training')\n",
        "    parser.add_argument('--no_cuda',        action='store_true')\n",
        "    parser.add_argument('--distributed',    action='store_true')\n",
        "    # Model parameters\n",
        "    parser.add_argument('--hidden',         default=768,  type=int,   help='the number of expected features in the transformer decoder')\n",
        "    parser.add_argument('--n_layers',       default=12,   type=int,   help='the number of decoder layers')\n",
        "    parser.add_argument('--n_attn_heads',   default=12,   type=int,   help='the number of multi-head attention heads')\n",
        "    parser.add_argument('--embd_dropout',   default=0.1,  type=float, help='embedding dropout value')\n",
        "    parser.add_argument('--resid_dropout',  default=0.1,  type=float, help='residual dropout value')\n",
        "    parser.add_argument('--attn_dropout',   default=0.1,  type=float, help='attention dropout value')\n",
        "    parser.add_argument('--ffn_hidden',     default=3072, type=int,   help='dimension of the feedforward network')\n",
        "    # Others\n",
        "    parser.add_argument('--cached_label_dict', default='cached_label_dict.json', type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "9zH3FwHr3DA5",
        "outputId": "f209f286-79da-46cf-c73c-22388232d832"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --train_corpus TRAIN_CORPUS --vocab_file\n",
            "                                VOCAB_FILE --pretrained_sp_model\n",
            "                                PRETRAINED_SP_MODEL [--pretrain] [--finetune]\n",
            "                                [--do_eval] [--test_corpus TEST_CORPUS]\n",
            "                                [--pretrained_model PRETRAINED_MODEL]\n",
            "                                [--output_model_prefix OUTPUT_MODEL_PREFIX]\n",
            "                                [--batch_size BATCH_SIZE]\n",
            "                                [--max_seq_len MAX_SEQ_LEN]\n",
            "                                [--n_workers N_WORKERS] [--epochs EPOCHS]\n",
            "                                [--lr LR] [--auxiliary_ratio AUXILIARY_RATIO]\n",
            "                                [--local_rank LOCAL_RANK] [--no_cuda]\n",
            "                                [--distributed] [--hidden HIDDEN]\n",
            "                                [--n_layers N_LAYERS]\n",
            "                                [--n_attn_heads N_ATTN_HEADS]\n",
            "                                [--embd_dropout EMBD_DROPOUT]\n",
            "                                [--resid_dropout RESID_DROPOUT]\n",
            "                                [--attn_dropout ATTN_DROPOUT]\n",
            "                                [--ffn_hidden FFN_HIDDEN]\n",
            "                                [--cached_label_dict CACHED_LABEL_DICT]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --train_corpus, --vocab_file, --pretrained_sp_model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    }
  ]
}