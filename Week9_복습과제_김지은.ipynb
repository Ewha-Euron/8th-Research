{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **PyTorch로 GPT 구현**"
      ],
      "metadata": {
        "id": "HPLeQXp5D_7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### data_utils.py\n",
        "\t•\t텍스트 코퍼스를 읽어 학습용 입력 예시로 변환함\n",
        "\t•\t토크나이저를 이용해 문장을 토큰화하고 BOS, EOS, PAD 토큰 추가\n",
        "\t•\t토큰을 ID로 변환해 input_ids 생성\n",
        "\t•\t사전학습 모드에서는 텍스트만, 미세조정 모드에서는 텍스트와 라벨 쌍 처리\n",
        "\t•\t미세조정 모드의 학습 데이터 라벨은 딕셔너리로 매핑하여 ID 부여\n",
        "\t•\t변환된 결과는 캐시 파일로 저장하거나 기존 캐시에서 불러옴\n",
        "\t•\t최종적으로 PyTorch TensorDataset 형태로 변환해 반환함"
      ],
      "metadata": {
        "id": "YL7d2UldR2My"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterable, Union, List\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "class PretrainInputExample:\n",
        "    def __init__(self, text: str):\n",
        "        self.text = text\n",
        "\n",
        "class ClsInputExample:\n",
        "    def __init__(self, text: str, label: str):\n",
        "        self.text = text\n",
        "        self.label = label\n",
        "\n",
        "class PretrainInputFeatures:\n",
        "    def __init__(self, input_ids: List[int]):\n",
        "        self.input_ids = input_ids\n",
        "\n",
        "class ClsInputFeatures:\n",
        "    def __init__(self, input_ids: List[int], label_id: int):\n",
        "        self.input_ids = input_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "def convert_examples_to_features(examples,\n",
        "                                 tokenizer,\n",
        "                                 args,\n",
        "                                 mode):\n",
        "    bos_token = tokenizer.bos_token\n",
        "    eos_token = tokenizer.eos_token\n",
        "    pad_token = tokenizer.pad_token\n",
        "    if args.finetune:\n",
        "        if mode == 'train':\n",
        "            labels = sorted(list(set([example.label for example in examples])))\n",
        "            label_dict = {label: i for i, label in enumerate(labels)}\n",
        "            with open(args.cached_label_dict, 'w') as file:\n",
        "                json.dump(label_dict, file,  indent=4)\n",
        "        elif mode == 'test':\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "\n",
        "    features = []\n",
        "    for i, example in enumerate(examples):\n",
        "        tokens = tokenizer.tokenize(example.text)\n",
        "        tokens = [bos_token] + tokens[:args.max_seq_len-2] + [eos_token] # BOS, EOS\n",
        "        tokens += [pad_token] * (args.max_seq_len - len(tokens))\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        if args.finetune:\n",
        "            label_id = label_dict.get(example.label)\n",
        "\n",
        "        if args.pretrain:\n",
        "            feature = PretrainInputFeatures(input_ids)\n",
        "        elif args.finetune:\n",
        "            feature = ClsInputFeatures(input_ids, label_id)\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    return features\n",
        "\n",
        "def create_examples(args, tokenizer, mode='train'):\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        dist.barrier()\n",
        "    assert mode in ('train', 'test')\n",
        "    cached_features_file = Path('cached_features_{}_{}_{}'.format('pretrain' if args.pretrain else 'finetune', mode, args.max_seq_len))\n",
        "\n",
        "    if cached_features_file.exists():\n",
        "        print('Loading features from cached file', cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        corpus_path = args.train_corpus if mode=='train' else args.test_corpus\n",
        "        with open(corpus_path, 'r', encoding='utf-8') as reader:\n",
        "            corpus = reader.readlines()\n",
        "\n",
        "        if args.pretrain:\n",
        "            corpus = list(map(lambda x: x.strip(), corpus))\n",
        "            corpus = list(filter(lambda x: len(x) > 0, corpus))\n",
        "            examples = [PretrainInputExample(text) for text in corpus]\n",
        "        elif args.finetune:\n",
        "            corpus = list(map(lambda x: x.split('\\t'), corpus))\n",
        "            corpus = list(map(lambda x: list(map(lambda y: y.strip(), x)), corpus))\n",
        "            corpus = list(map(lambda x: list(filter(lambda y: len(y) > 0, x)), corpus))\n",
        "            examples = [ClsInputExample(text, label) for label, text in corpus]\n",
        "\n",
        "        features = convert_examples_to_features(examples, tokenizer, args, mode)\n",
        "\n",
        "        print('Saving features into cached file', cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        dist.barrier()\n",
        "\n",
        "    if args.pretrain:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids)\n",
        "    elif args.finetune:\n",
        "        all_input_ids = torch.tensor([feature.input_ids for feature in features], dtype=torch.long)\n",
        "        all_label_ids = torch.tensor([feature.label_id for feature in features], dtype=torch.long)\n",
        "        dataset = TensorDataset(all_input_ids, all_label_ids)\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "Yc7jyBOvR48X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model.py\n",
        "\t•\tScaledDotProductAttention: 쿼리와 키의 내적을 스케일링하고 마스킹 및 softmax를 적용해 가중치를 계산\n",
        "\t•\tMultiHeadAttention: 입력을 여러 헤드로 나누어 병렬로 attention을 계산하고 출력 결합\n",
        "\t•\tPositionWiseFeedForwardNetwork: 각 위치별로 독립적으로 feed-forward 연산을 수행\n",
        "\t•\tDecoderLayer: multi-head attention과 feed-forward 네트워크로 구성된 디코더 블록\n",
        "\t•\tTransformerDecoder: 임베딩과 positional embedding 후 여러 디코더 레이어를 순차적으로 적용\n",
        "\t•\tGPT: transformer decoder로 구성된 GPT 모델 정의\n",
        "\t•\tGPTLMHead: GPT 출력에 linear projection을 적용해 언어 모델링 logits 생성\n",
        "\t•\tGPTClsHead: GPT 출력에서 특정 토큰 위치 벡터를 추출해 분류 logits 생성하고 language modeling과 함께 반환"
      ],
      "metadata": {
        "id": "jjqbUvOcMmuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iVYcnyObCb4c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, d_k, attn_pdrop):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.d_k = d_k\n",
        "\n",
        "        self.dropout = nn.Dropout(attn_pdrop)\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "        attn_score = torch.matmul(q, k.transpose(-1, -2)) / (self.d_k ** 0.5)\n",
        "        attn_score.masked_fill_(attn_mask, -1e9)\n",
        "\n",
        "        attn_weights = nn.Softmax(dim=-1)(attn_score)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        output = torch.matmul(attn_weights, v)\n",
        "\n",
        "        return output, attn_weights\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, attn_pdrop):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "\n",
        "        self.WQ = nn.Linear(d_model, d_model)\n",
        "        self.WK = nn.Linear(d_model, d_model)\n",
        "        self.WV = nn.Linear(d_model, d_model)\n",
        "        self.scaled_dot_product_attn = ScaledDotProductAttention(self.d_k, attn_pdrop)\n",
        "        self.linear = nn.Linear(n_heads * self.d_v, d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        q_heads = self.WQ(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        k_heads = self.WK(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        v_heads = self.WV(V).view(batch_size, -1, self.n_heads, self.d_v).transpose(1, 2)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.n_heads, 1, 1)\n",
        "        attn, attn_weights = self.scaled_dot_product_attn(q_heads, k_heads, v_heads, attn_mask)\n",
        "\n",
        "        attn = attn.transpose(1, 2).contiguous().view(batch_size, -1, self.n_heads * self.d_v)\n",
        "        outputs = self.linear(attn)\n",
        "\n",
        "        return outputs, attn_weights\n",
        "\n",
        "class PositionWiseFeedForwardNetwork(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForwardNetwork, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "        nn.init.normal_(self.linear1.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs = self.gelu(self.linear1(inputs))\n",
        "        outputs = self.linear2(outputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, attn_pdrop, resid_pdrop):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, n_heads, attn_pdrop)\n",
        "        self.dropout1 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm1 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "        self.ffn = PositionWiseFeedForwardNetwork(d_model, d_ff)\n",
        "        self.dropout2 = nn.Dropout(resid_pdrop)\n",
        "        self.layernorm2 = nn.LayerNorm(d_model, eps=1e-5)\n",
        "\n",
        "    def forward(self, inputs, attn_mask):\n",
        "        attn_outputs, attn_weights = self.mha(inputs, inputs, inputs, attn_mask)\n",
        "        attn_outputs = self.dropout1(attn_outputs)\n",
        "        attn_outputs = self.layernorm1(inputs + attn_outputs)\n",
        "\n",
        "        ffn_outputs = self.ffn(attn_outputs)\n",
        "        ffn_outputs = self.dropout2(ffn_outputs)\n",
        "        ffn_outputs = self.layernorm2(attn_outputs + ffn_outputs)\n",
        "\n",
        "        return ffn_outputs, attn_weights\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, seq_len, d_model, n_layers, n_heads, d_ff, embd_pdrop, attn_pdrop, resid_pdrop, pad_id):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "        self.pad_id = pad_id\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.dropout = nn.Dropout(embd_pdrop)\n",
        "        self.pos_embedding = nn.Embedding(seq_len+1, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff, attn_pdrop, resid_pdrop) for _ in range(n_layers)])\n",
        "\n",
        "        nn.init.normal_(self.embedding.weight, std=0.02)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        positions = torch.arange(inputs.size(1), device=inputs.device, dtype=inputs.dtype).repeat(inputs.size(0), 1) + 1\n",
        "        position_pad_mask = inputs.eq(self.pad_id)\n",
        "        positions.masked_fill_(position_pad_mask, 0)\n",
        "\n",
        "        outputs = self.dropout(self.embedding(inputs)) + self.pos_embedding(positions)\n",
        "\n",
        "        attn_pad_mask = self.get_attention_padding_mask(inputs, inputs, self.pad_id)\n",
        "        subsequent_mask = self.get_attention_subsequent_mask(inputs).to(device=attn_pad_mask.device)\n",
        "        attn_mask = torch.gt((attn_pad_mask.to(dtype=subsequent_mask.dtype) + subsequent_mask), 0)\n",
        "\n",
        "        attention_weights = []\n",
        "        for layer in self.layers:\n",
        "            outputs, attn_weights = layer(outputs, attn_mask)\n",
        "            attention_weights.append(attn_weights)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "    def get_attention_padding_mask(self, q, k, pad_id):\n",
        "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
        "\n",
        "        return attn_pad_mask\n",
        "\n",
        "    def get_attention_subsequent_mask(self, q):\n",
        "        bs, q_len = q.size()\n",
        "        subsequent_mask = torch.ones(bs, q_len, q_len).triu(diagonal=1)\n",
        "\n",
        "        return subsequent_mask\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 seq_len=512,\n",
        "                 d_model=768,\n",
        "                 n_layers=12,\n",
        "                 n_heads=12,\n",
        "                 d_ff=3072,\n",
        "                 embd_pdrop=0.1,\n",
        "                 attn_pdrop=0.1,\n",
        "                 resid_pdrop=0.1,\n",
        "                 pad_id=0):\n",
        "        super(GPT, self).__init__()\n",
        "\n",
        "        self.decoder = TransformerDecoder(vocab_size, seq_len, d_model, n_layers, n_heads, d_ff,\n",
        "                                          embd_pdrop, attn_pdrop, resid_pdrop, pad_id)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.decoder(inputs)\n",
        "\n",
        "        return outputs, attention_weights\n",
        "\n",
        "class GPTLMHead(nn.Module):\n",
        "    def __init__(self, gpt):\n",
        "        super(GPTLMHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "\n",
        "        self.gpt = gpt\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "        lm_logits = self.linear(outputs)\n",
        "\n",
        "        return lm_logits\n",
        "\n",
        "class GPTClsHead(nn.Module):\n",
        "    def __init__(self, gpt, n_class, cls_token_id, cls_pdrop=0.1):\n",
        "        super(GPTClsHead, self).__init__()\n",
        "        vocab_size, d_model = gpt.decoder.embedding.weight.size()\n",
        "        self.cls_token_id = cls_token_id\n",
        "\n",
        "        self.gpt = gpt\n",
        "\n",
        "        # LM\n",
        "        self.linear1 = nn.Linear(d_model, vocab_size, bias=False)\n",
        "        self.linear1.weight = gpt.decoder.embedding.weight\n",
        "\n",
        "        # Classification\n",
        "        self.linear2 = nn.Linear(d_model, n_class)\n",
        "        self.dropout = nn.Dropout(cls_pdrop)\n",
        "\n",
        "        nn.init.normal_(self.linear2.weight, std=0.02)\n",
        "        nn.init.normal_(self.linear2.bias, 0)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        outputs, attention_weights = self.gpt(inputs)\n",
        "\n",
        "        lm_logits = self.linear1(outputs)\n",
        "\n",
        "        outputs = outputs[inputs.eq(self.cls_token_id)]\n",
        "        cls_logits = self.linear2(self.dropout(outputs))\n",
        "\n",
        "        return lm_logits, cls_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tokenization.py\n",
        "\t•\tTokenizer: 문자열을 토큰으로 분할하고 토큰-ID 변환 기능을 제공\n",
        "\t•\t생성 시 vocab 파일을 읽어 토큰과 ID 간 매핑 딕셔너리 생성\n",
        "\t•\ttokenize: 입력 문자열을 토큰 목록으로 분할\n",
        "\t•\tconvert_token_to_id: 토큰을 ID로 변환\n",
        "\t•\tconvert_id_to_token: ID를 토큰으로 변환\n",
        "\t•\tconvert_tokens_to_ids: 토큰 리스트를 ID 리스트로 변환\n",
        "\t•\tconvert_ids_to_tokens: ID 리스트를 토큰 리스트로 변환\n",
        "\t•\t여러 특수 토큰의 ID 속성 제공\n",
        "\t•\tPretrainedTokenizer: 사전학습된 SentencePiece 모델을 기반으로 Tokenizer 초기화\n",
        "\t•\tdetokenize: 토큰 리스트를 문자열로 복원"
      ],
      "metadata": {
        "id": "yJdR9KMESSFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install prenlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tin1QhgYSZwM",
        "outputId": "268fbccc-29ba-4a7f-ae1e-5a19d386c0eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting prenlp\n",
            "  Downloading prenlp-0.0.13-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting nltk==3.2.5 (from prenlp)\n",
            "  Downloading nltk-3.2.5.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting konlpy (from prenlp)\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from prenlp) (0.2.0)\n",
            "Collecting ijson (from prenlp)\n",
            "  Downloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting py7zr==0.5b5 (from prenlp)\n",
            "  Downloading py7zr-0.5b5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from nltk==3.2.5->prenlp) (1.17.0)\n",
            "Collecting texttable (from py7zr==0.5b5->prenlp)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy->prenlp)\n",
            "  Downloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy->prenlp) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy->prenlp) (24.2)\n",
            "Downloading prenlp-0.0.13-py3-none-any.whl (30 kB)\n",
            "Downloading py7zr-0.5b5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.0/135.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.1/494.1 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.5-py3-none-any.whl size=1392144 sha256=e22330c70f56bf437854915a83ab9d623283693a632ddb3df80dbed3b4584120\n",
            "  Stored in directory: /root/.cache/pip/wheels/0e/69/e3/8b11e6490c8f20fcab5f6a3321d60fcc0b26ed6f7745ad95b4\n",
            "Successfully built nltk\n",
            "Installing collected packages: texttable, py7zr, nltk, JPype1, ijson, konlpy, prenlp\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.2.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JPype1-1.5.2 ijson-3.4.0 konlpy-0.6.0 nltk-3.2.5 prenlp-0.0.13 py7zr-0.5b5 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from collections import OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, tokenizer, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pad_token = pad_token\n",
        "        self.unk_token = unk_token\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "        self.sep_token = sep_token\n",
        "        self.cls_token = cls_token\n",
        "        self.mask_token = mask_token\n",
        "        self.vocab = OrderedDict()\n",
        "        self.ids_to_tokens = OrderedDict()\n",
        "\n",
        "        with open(vocab_file, 'r', encoding='utf-8') as reader:\n",
        "            for i, line in enumerate(reader.readlines()):\n",
        "                token = line.split()[0]\n",
        "                self.vocab[token] = i\n",
        "        for token, id in self.vocab.items():\n",
        "            self.ids_to_tokens[id] = token\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        return self.tokenizer(text)\n",
        "\n",
        "    def convert_token_to_id(self, token: str) -> int:\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def convert_id_to_token(self, id: int) -> str:\n",
        "        return self.ids_to_tokens.get(id, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens: List[str]) -> List[int]:\n",
        "        return [self.convert_token_to_id(token) for token in tokens]\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids: List[int]) -> List[str]:\n",
        "        return [self.convert_id_to_token(id) for id in ids]\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return len(self.vocab)\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.pad_token)\n",
        "\n",
        "    @property\n",
        "    def unk_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.bos_token)\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.eos_token)\n",
        "\n",
        "    @property\n",
        "    def sep_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.sep_token)\n",
        "\n",
        "    @property\n",
        "    def cls_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.cls_token)\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self) -> int:\n",
        "        return self.convert_token_to_id(self.mask_token)\n",
        "\n",
        "class PretrainedTokenizer(Tokenizer):\n",
        "    def __init__(self, pretrained_model: str, vocab_file: str,\n",
        "                 pad_token: str = '[PAD]',\n",
        "                 unk_token: str = '[UNK]',\n",
        "                 bos_token: str = '[BOS]',\n",
        "                 eos_token: str = '[EOS]',\n",
        "                 sep_token: str = '[SEP]',\n",
        "                 cls_token: str = '[CLS]',\n",
        "                 mask_token: str = '[MASK]'):\n",
        "        tokenizer = SentencePiece.load(pretrained_model)\n",
        "\n",
        "        super(PretrainedTokenizer, self).__init__(tokenizer, vocab_file, pad_token, unk_token, bos_token, eos_token)\n",
        "\n",
        "    def detokenize(self, tokens: List[str]) -> str:\n",
        "        return self.tokenizer.detokenize(tokens)"
      ],
      "metadata": {
        "id": "4XJ2FfTlST-u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### vocab.py\n",
        "\t•\t--corpus: 한 줄에 하나의 문장이 있는 입력 텍스트 파일 경로 지정\n",
        "\t•\t--prefix: 생성될 vocab 및 모델 파일 이름의 접두어 지정 ({prefix}.model, {prefix}.vocab)\n",
        "\t•\t--vocab_size: 생성할 서브워드 vocab의 크기 설정\n",
        "\t•\t--character_coverage: 문자 커버리지 비율 설정\n",
        "\t•\t--model_type: 사용할 SentencePiece 모델 타입 선택\n",
        "\t•\t--max_sentence_length: 학습에 사용할 최대 문장 길이 제한\n",
        "\t•\t--pad_token, --unk_token, --bos_token, --eos_token: 특수 토큰 정의\n",
        "\t•\tbuild(args): 위 인자들을 기반으로 SentencePiece.train()을 호출하여 모델 학습 수행\n",
        "\t•\t실행 시 python build_tokenizer.py --corpus my.txt --prefix mytokenizer와 같이 사용 가능"
      ],
      "metadata": {
        "id": "vJlEa4lsTCO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from prenlp.tokenizer import SentencePiece\n",
        "\n",
        "def build(args):\n",
        "    tokenizer = SentencePiece.train(input = args.corpus, model_prefix = args.prefix,\n",
        "                                    vocab_size = args.vocab_size,\n",
        "                                    model_type = args.model_type,\n",
        "                                    character_coverage = args.character_coverage,\n",
        "                                    max_sentence_length = args.max_sentence_length,\n",
        "                                    pad_token = args.pad_token,\n",
        "                                    unk_token = args.unk_token,\n",
        "                                    bos_token = args.bos_token,\n",
        "                                    eos_token = args.eos_token)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--corpus',      required=True,           type=str, help='one-sentence-per-line corpus file')\n",
        "    parser.add_argument('--prefix',      required=True,           type=str, help='output vocab(or sentencepiece model) name prefix')\n",
        "\n",
        "    parser.add_argument('--vocab_size',          default=16000,   type=int, help='the maximum size of the vocabulary')\n",
        "    parser.add_argument('--character_coverage',  default=1.0,     type=float,\n",
        "                        help='amount of characters covered by the model, good defaults are: 0.9995 for languages with rich character set\\\n",
        "                             like Japanse or Chinese and 1.0 for other languages with small character set')\n",
        "    parser.add_argument('--model_type',          default='bpe',   type=str, help='sentencepiece model type. Choose from unigram, bpe, char, or word')\n",
        "    parser.add_argument('--max_sentence_length', default=100000,  type=int, help='The maximum input sequence length')\n",
        "    parser.add_argument('--pad_token',           default='[PAD]', type=str, help='token that indicates padding')\n",
        "    parser.add_argument('--unk_token',           default='[UNK]', type=str, help='token that indicates unknown word')\n",
        "    parser.add_argument('--bos_token',           default='[BOS]', type=str, help='token that indicates beginning of sentence')\n",
        "    parser.add_argument('--eos_token',           default='[EOS]', type=str, help='token that indicates end of sentence')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    build(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "3_ESs0AYTE1l",
        "outputId": "a04397eb-15ce-4889-83b2-32bbd48829e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --corpus CORPUS --prefix PREFIX\n",
            "                                [--vocab_size VOCAB_SIZE]\n",
            "                                [--character_coverage CHARACTER_COVERAGE]\n",
            "                                [--model_type MODEL_TYPE]\n",
            "                                [--max_sentence_length MAX_SENTENCE_LENGTH]\n",
            "                                [--pad_token PAD_TOKEN]\n",
            "                                [--unk_token UNK_TOKEN]\n",
            "                                [--bos_token BOS_TOKEN]\n",
            "                                [--eos_token EOS_TOKEN]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --corpus, --prefix\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### trainer.py"
      ],
      "metadata": {
        "id": "Il2VDfCeKD4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pytorch-optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YTXX72OjNF1f",
        "outputId": "26d20d4b-0049-4e5d-cdc8-2c32ddfe6075"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-optimizer\n",
            "  Downloading pytorch_optimizer-3.5.1-py3-none-any.whl.metadata (71 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-optimizer) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from pytorch-optimizer) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10->pytorch-optimizer)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->pytorch-optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->pytorch-optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->pytorch-optimizer) (3.0.2)\n",
            "Downloading pytorch_optimizer-3.5.1-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.8/240.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-optimizer\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-optimizer-3.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "d01c5cdf33e34faf94f9d879ed430014"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnOJOn99PA0t",
        "outputId": "39e787ea-c464-4819-9e6e-7457851fe8d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.3.0-py3-none-any.whl.metadata (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torch_optimizer) (2.6.0+cu124)\n",
            "Collecting pytorch-ranger>=0.1.1 (from torch_optimizer)\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl.metadata (509 bytes)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torch_optimizer) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torch_optimizer) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torch_optimizer) (3.0.2)\n",
            "Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytorch-ranger, torch_optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch_optimizer-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\t•\tTrainer: GPT 모델의 학습, 평가, 저장을 담당하는 클래스 정의\n",
        "\t•\t__init__: 학습 인자, 데이터 로더, 토크나이저를 받아 모델과 옵티마이저 초기화\n",
        "\t•\t사전학습 모드에서는 GPTLMHead, 미세조정 모드에서는 GPTClsHead 모델 사용\n",
        "\t•\t분산 학습(distributed training)을 지원하며, 해당 시 DistributedDataParallel로 모델 감쌈\n",
        "\t•\ttrain: 한 epoch 동안 학습 수행 (모드에 따라 pretrain 또는 finetune 호출)\n",
        "\t•\tpretrain: 입력 문장에 대해 다음 토큰 예측을 위한 GPT 언어 모델 학습\n",
        "\t•\tfinetune: 입력 문장과 라벨을 이용해 언어 모델링 + 분류 모델 학습\n",
        "\t•\tevaluate: 학습된 모델을 검증 데이터셋에 대해 평가, loss 및 accuracy 기록\n",
        "\t•\tsave: 현재 모델 상태를 지정된 경로에 저장\n",
        "\t•\t@timeit: 함수 실행 시간을 측정하고 출력하는 데코레이터 정의"
      ],
      "metadata": {
        "id": "74jKzIZeUv7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch_optimizer import RAdam\n",
        "\n",
        "def timeit(method):\n",
        "    def timed(*args, **kw):\n",
        "        _args = args[0].args\n",
        "\n",
        "        ts = time.time()\n",
        "        result = method(*args, **kw)\n",
        "        te = time.time()\n",
        "\n",
        "        if _args.distributed:\n",
        "            if _args.local_rank == 0:\n",
        "                print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "        else:\n",
        "            print('Function Time: {}\\t>\\t{:.0f} min {:.0f} sec'.format(method.__name__, (te-ts)//60, (te-ts)%60))\n",
        "\n",
        "        return result\n",
        "    return timed\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, args, train_loader, test_loader, tokenizer):\n",
        "        self.args = args\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.pad_id = tokenizer.pad_token_id\n",
        "        self.eos_id = tokenizer.eos_token_id\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu', args.local_rank)\n",
        "        self.writer = SummaryWriter() if args.local_rank in [-1, 0] else None\n",
        "        self.n_gpus = torch.distributed.get_world_size() if args.distributed else torch.cuda.device_count()\n",
        "        assert args.pretrain != args.finetune\n",
        "\n",
        "        if args.pretrained_model:\n",
        "            self.gpt = torch.load(args.pretrained_model)\n",
        "        else:\n",
        "            self.gpt = GPT(vocab_size=self.vocab_size,\n",
        "                           seq_len=args.max_seq_len,\n",
        "                           d_model=args.hidden,\n",
        "                           n_layers=args.n_layers,\n",
        "                           n_heads=args.n_attn_heads,\n",
        "                           d_ff=args.ffn_hidden,\n",
        "                           embd_pdrop=args.embd_dropout,\n",
        "                           attn_pdrop=args.attn_dropout,\n",
        "                           resid_pdrop=args.resid_dropout,\n",
        "                           pad_id=self.pad_id)\n",
        "\n",
        "        if args.pretrain:\n",
        "            self.model = GPTLMHead(self.gpt)\n",
        "            self.model.to(self.device)\n",
        "        if args.finetune:\n",
        "            with open(args.cached_label_dict, 'r') as file:\n",
        "                label_dict = json.load(file)\n",
        "            self.model = GPTClsHead(self.gpt, n_class=len(label_dict), cls_token_id=self.eos_id)\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        if args.distributed:\n",
        "            self.model = DistributedDataParallel(self.model, device_ids=[args.local_rank], output_device=args.local_rank)\n",
        "\n",
        "        self.optimizer = RAdam(self.model.parameters(), args.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index = self.pad_id).to(self.device)\n",
        "        self.cls_criterion = nn.CrossEntropyLoss().to(self.device)\n",
        "\n",
        "    @timeit\n",
        "    def train(self, epoch):\n",
        "        if self.args.pretrain:\n",
        "            self.pretrain(epoch)\n",
        "        if self.args.finetune:\n",
        "            self.finetune(epoch)\n",
        "\n",
        "    def pretrain(self, epoch):\n",
        "        losses = 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs = batch[0].to(self.device)\n",
        "            targets = inputs[:, 1:].contiguous()\n",
        "            lm_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "            losses += loss.item()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/pre-train', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f}'.format(i, i, n_batches, losses/i))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f}'.format(epoch, self.args.local_rank, losses/n_batches))\n",
        "\n",
        "    def finetune(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.train_loader), len(self.train_loader.dataset)\n",
        "\n",
        "        self.model.train()\n",
        "        for i, batch in enumerate(self.train_loader):\n",
        "            inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "            lm_logits, cls_logits = self.model(inputs)\n",
        "            lm_logits = lm_logits[:, :-1].contiguous()\n",
        "            lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "            cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "            loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "            losses += loss.item()\n",
        "            acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "            accs += acc\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if self.args.local_rank in [-1, 0]:\n",
        "                self.writer.add_scalar('Loss/fine-tune', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                self.writer.add_scalar('Accuracy/fine-tune', acc, ((epoch-1)*n_batches)+i)\n",
        "                if i % (n_batches//5) == 0 and i != 0:\n",
        "                    print('Iteration {} ({}/{})\\tLoss: {:.4f} Acc: {:.1f}%'.format(i, i, n_batches, losses/i, accs/i*100.))\n",
        "\n",
        "        print('Train Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def evaluate(self, epoch):\n",
        "        losses, accs = 0, 0\n",
        "        n_batches, n_samples = len(self.test_loader), len(self.test_loader.dataset)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(self.test_loader):\n",
        "                if self.args.pretrain:\n",
        "                    inputs = batch.to(self.device)\n",
        "                    targets = inputs[:, 1:].contiguous()\n",
        "\n",
        "                    lm_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    loss = self.criterion(lm_logits.view(-1, self.vocab_size), targets.view(-1))\n",
        "                    losses += loss.item()\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/pre-train(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "\n",
        "                elif self.args.finetune:\n",
        "                    inputs, labels = map(lambda x: x.to(self.device), batch)\n",
        "\n",
        "                    lm_logits, cls_logits = self.model(inputs)\n",
        "                    lm_logits = lm_logits[:, :-1].contiguous()\n",
        "\n",
        "                    lm_loss = self.criterion(lm_logits.view(-1, self.vocab_size), inputs[:, 1:].contiguous().view(-1))\n",
        "                    cls_loss = self.cls_criterion(cls_logits, labels)\n",
        "                    loss = cls_loss + (self.args.auxiliary_ratio * lm_loss)\n",
        "\n",
        "                    losses += loss.item()\n",
        "                    acc = (cls_logits.argmax(dim=-1) == labels).to(dtype=cls_logits.dtype).mean()\n",
        "                    accs += acc\n",
        "\n",
        "                    if self.args.local_rank in [-1, 0]:\n",
        "                        self.writer.add_scalar('Loss/fine-tune(eval)', loss.item(), ((epoch-1)*n_batches)+i)\n",
        "                        self.writer.add_scalar('Accuracy/fine-tune(eval)', acc, ((epoch-1)*n_batches)+i)\n",
        "\n",
        "        print('Eval Epoch {} [rank: {}]\\t>\\tLoss: {:.4f} / Acc: {:.1f}%'.format(epoch, self.args.local_rank, losses/n_batches, accs/n_batches*100.))\n",
        "\n",
        "    def save(self, epoch, model_prefix='model', root='.model'):\n",
        "        path = Path(root) / (model_prefix + '.ep%d' % epoch)\n",
        "        if not path.parent.exists():\n",
        "            path.parent.mkdir()\n",
        "\n",
        "        if self.args.distributed:\n",
        "            if self.args.local_rank == 0:\n",
        "                torch.save(self.gpt, path)\n",
        "        else:\n",
        "            torch.save(self.gpt, path)"
      ],
      "metadata": {
        "id": "D3BP5_FFK84Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### main.py\n",
        "\t•\targparse를 통해 학습 설정, 파일 경로, 모델 파라미터 등을 커맨드라인 인자로 받아 처리\n",
        "\t•\t--pretrain, --finetune 플래그로 실행 모드를 선택\n",
        "\t•\t분산 학습(distributed training) 설정 시 torch.distributed.init_process_group()으로 초기화\n",
        "\t•\tSentencePiece 기반의 PretrainedTokenizer 로드 및 학습용 데이터셋 생성\n",
        "\t•\tcreate_examples() 함수를 통해 입력 텍스트를 PyTorch Dataset으로 변환\n",
        "\t•\tDataLoader와 RandomSampler 또는 DistributedSampler로 배치 구성\n",
        "\t•\tTrainer 객체를 생성하여 학습/저장/평가 루프 실행\n",
        "\t•\ttrainer.train(), trainer.save(), trainer.evaluate()를 epoch마다 반복 수행\n",
        "\t•\t평가 모드 활성화(--do_eval) 시 테스트셋도 로딩하여 성능 측정"
      ],
      "metadata": {
        "id": "vI-qzimSRrko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler\n",
        "\n",
        "from data_utils import create_examples\n",
        "from tokenization import PretrainedTokenizer\n",
        "from trainer import Trainer\n",
        "\n",
        "def main(args):\n",
        "    print(args)\n",
        "    if args.distributed:\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        dist.init_process_group(backend='nccl')\n",
        "\n",
        "    tokenizer = PretrainedTokenizer(pretrained_model=args.pretrained_sp_model, vocab_file=args.vocab_file)\n",
        "    train_dataset = create_examples(args, tokenizer, mode='train')\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_loader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.batch_size, num_workers=args.n_workers)\n",
        "    if args.do_eval:\n",
        "        test_dataset = create_examples(args, tokenizer, mode='test')\n",
        "        test_sampler = RandomSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n",
        "        test_loader = DataLoader(test_dataset, sampler=test_sampler, batch_size=args.batch_size, num_workers=args.n_workers)\n",
        "\n",
        "    trainer = Trainer(args=args,\n",
        "                      train_loader=train_loader,\n",
        "                      test_loader=test_loader if args.do_eval else None,\n",
        "                      tokenizer=tokenizer)\n",
        "\n",
        "    for epoch in range(1, args.epochs+1):\n",
        "        trainer.train(epoch)\n",
        "        trainer.save(epoch, args.output_model_prefix)\n",
        "        if args.do_eval:\n",
        "            trainer.evaluate(epoch)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--train_corpus',           required=True,     type=str, help='corpus for either pre-train or fine-tune')\n",
        "    parser.add_argument('--vocab_file',             required=True,     type=str, help='pretrained vocabulary')\n",
        "    parser.add_argument('--pretrained_sp_model',    required=True,     type=str, help='pretrained sentencepiece model')\n",
        "    parser.add_argument('--pretrain',               action='store_true')\n",
        "    parser.add_argument('--finetune',               action='store_true')\n",
        "    parser.add_argument('--do_eval',                action='store_true')\n",
        "\n",
        "    parser.add_argument('--test_corpus',            default=None,     type=str, help='corpus for either pre-train or fine-tune evaluation')\n",
        "    parser.add_argument('--pretrained_model',       default=None,     type=str, help='pretrained GPT model path')\n",
        "    parser.add_argument('--output_model_prefix',    default='model',  type=str, help='output model name prefix')\n",
        "    # Input parameters\n",
        "    parser.add_argument('--batch_size',     default=64,    type=int,   help='batch size')\n",
        "    parser.add_argument('--max_seq_len',    default=512,   type=int,   help='the maximum size of the input sequence')\n",
        "    parser.add_argument('--n_workers',      default=4,     type=int,   help='the number of workers')\n",
        "    # Train parameters\n",
        "    parser.add_argument('--epochs',         default=100,       type=int,   help='the number of epochs')\n",
        "    parser.add_argument('--lr',             default=1.5e-4,    type=float, help='initial learning rate')\n",
        "    parser.add_argument('--auxiliary_ratio',default=.25,       type=float, help='weight of auxiliary objective')\n",
        "    parser.add_argument('--local_rank',     default=-1,        type=int,   help='node rank for distributed training')\n",
        "    parser.add_argument('--no_cuda',        action='store_true')\n",
        "    parser.add_argument('--distributed',    action='store_true')\n",
        "    # Model parameters\n",
        "    parser.add_argument('--hidden',         default=768,  type=int,   help='the number of expected features in the transformer decoder')\n",
        "    parser.add_argument('--n_layers',       default=12,   type=int,   help='the number of decoder layers')\n",
        "    parser.add_argument('--n_attn_heads',   default=12,   type=int,   help='the number of multi-head attention heads')\n",
        "    parser.add_argument('--embd_dropout',   default=0.1,  type=float, help='embedding dropout value')\n",
        "    parser.add_argument('--resid_dropout',  default=0.1,  type=float, help='residual dropout value')\n",
        "    parser.add_argument('--attn_dropout',   default=0.1,  type=float, help='attention dropout value')\n",
        "    parser.add_argument('--ffn_hidden',     default=3072, type=int,   help='dimension of the feedforward network')\n",
        "    # Others\n",
        "    parser.add_argument('--cached_label_dict', default='cached_label_dict.json', type=str)\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    main(args)"
      ],
      "metadata": {
        "id": "D6bL3hyTRs5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}