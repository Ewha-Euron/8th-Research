{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUhIWgkOajyJ",
        "outputId": "8df3de96-d7ae-47df-8526-1dceab5b39b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Euron"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiVH2xTwa9Za",
        "outputId": "8f28bc32-4921-4f21-cc5a-0bb174f7c0f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Euron\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDIN0NG9cLKs",
        "outputId": "ad550e3d-27b8-42e7-cc2e-847cea56dfa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# swin_transformer.py\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# Shifted Window Attention 구현\n",
        "class CyclicShift(nn.Module):\n",
        "    def __init__(self, displacement):\n",
        "        super().__init__()\n",
        "        self.displacement = displacement\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.roll(x, shifts=(self.displacement, self.displacement), dims=(1, 2))\n",
        "\n",
        "# Residual Connection : 주어진 함수의 출력(self.fn)에 입력 x를 더함\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(x, **kwargs) + x\n",
        "\n",
        "# LayerNorm 먼저 적용 -> fn에 입력\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "# Attention 과정 적용 이후\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Shifted window attention에서 경계 간의 연결을 차단하는 마스크 생성\n",
        "def create_mask(window_size, displacement, upper_lower, left_right):\n",
        "    mask = torch.zeros(window_size ** 2, window_size ** 2)\n",
        "\n",
        "    if upper_lower:\n",
        "        mask[-displacement * window_size:, :-displacement * window_size] = float('-inf')\n",
        "        mask[:-displacement * window_size, -displacement * window_size:] = float('-inf')\n",
        "\n",
        "    if left_right:\n",
        "        mask = rearrange(mask, '(h1 w1) (h2 w2) -> h1 w1 h2 w2', h1=window_size, h2=window_size)\n",
        "        mask[:, -displacement:, :, :-displacement] = float('-inf')\n",
        "        mask[:, :-displacement, :, -displacement:] = float('-inf')\n",
        "        mask = rearrange(mask, 'h1 w1 h2 w2 -> (h1 w1) (h2 w2)')\n",
        "\n",
        "    return mask\n",
        "\n",
        "# 윈도우 내의 상대 위치 정보 계산 -> positive embedding에 사용\n",
        "def get_relative_distances(window_size):\n",
        "    indices = torch.tensor(np.array([[x, y] for x in range(window_size) for y in range(window_size)]))\n",
        "    distances = indices[None, :, :] - indices[:, None, :]\n",
        "    return distances"
      ],
      "metadata": {
        "id": "cLsO3nurbzjh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Swin Transformer의 핵심 구조 - 입력 이미지를 윈도우 단위로 나눈 뒤 각각에 대해 Multihead Self-attention 수행\n",
        "class WindowAttention(nn.Module):\n",
        "    def __init__(self, dim, heads, head_dim, img_size, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        inner_dim = head_dim * heads\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.window_size = window_size\n",
        "        self.relative_pos_embedding = relative_pos_embedding\n",
        "        self.shifted = shifted\n",
        "\n",
        "        if self.shifted:\n",
        "            displacement = window_size // 2\n",
        "            self.cyclic_shift = CyclicShift(-displacement)\n",
        "            self.cyclic_back_shift = CyclicShift(displacement)\n",
        "            # 마스크 생성 부분\n",
        "            self.upper_lower_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                             upper_lower=True, left_right=False), requires_grad=False)\n",
        "            self.left_right_mask = nn.Parameter(create_mask(window_size=window_size, displacement=displacement,\n",
        "                                                            upper_lower=False, left_right=True), requires_grad=False)\n",
        "\n",
        "        # 입력 feature : Q, K, V\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "\n",
        "        # 상대 위치 정보 담은 텐서 생성\n",
        "        if self.relative_pos_embedding:\n",
        "            self.relative_indices = get_relative_distances(window_size) + window_size - 1\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(2 * window_size - 1, 2 * window_size - 1))\n",
        "        else:\n",
        "            self.pos_embedding = nn.Parameter(torch.randn(window_size ** 2, window_size ** 2))\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력된 feature map shift\n",
        "        if self.shifted:\n",
        "            x = self.cyclic_shift(x)\n",
        "\n",
        "        # Q, K, V로 변환\n",
        "        b, n_h, n_w, _, h = *x.shape, self.heads\n",
        "\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        nw_h = n_h // self.window_size\n",
        "        nw_w = n_w // self.window_size\n",
        "\n",
        "        # 윈도우 단위로 배열 -> 윈도우별로 attention 계산 가능하게\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, 'b (nw_h w_h) (nw_w w_w) (h d) -> b h (nw_h nw_w) (w_h w_w) d',\n",
        "                                h=h, w_h=self.window_size, w_w=self.window_size), qkv)\n",
        "\n",
        "        dots = einsum('b h w i d, b h w j d -> b h w i j', q, k) * self.scale\n",
        "\n",
        "        if self.relative_pos_embedding: # relative_pos_embedding : 추가 위치 정보\n",
        "            dots += self.pos_embedding[self.relative_indices[:, :, 0], self.relative_indices[:, :, 1]] # 위치 정보 추가(Q, K 기반으로)\n",
        "        else:\n",
        "            dots += self.pos_embedding\n",
        "\n",
        "        # shifted = True\n",
        "        if self.shifted:\n",
        "            dots[:, :, -nw_w:] += self.upper_lower_mask\n",
        "            dots[:, :, nw_w - 1::nw_w] += self.left_right_mask\n",
        "\n",
        "        attn = dots.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b h w i j, b h w j d -> b h w i d', attn, v)\n",
        "\n",
        "        # 다시 원위치로 복원\n",
        "        out = rearrange(out, 'b h (nw_h nw_w) (w_h w_w) d -> b (nw_h w_h) (nw_w w_w) (h d)',\n",
        "                        h=h, w_h=self.window_size, w_w=self.window_size, nw_h=nw_h, nw_w=nw_w)\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        # 역시프트 과정 => shifted 경우\n",
        "        if self.shifted:\n",
        "            out = self.cyclic_back_shift(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "wrOmTqJYe3xH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 윈도우 기반 Attention + MLP + Residual 구조\n",
        "\n",
        "class SwinBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, img_size, head_dim, mlp_dim, shifted, window_size, relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        # PreNorm : LayerNorm 먼저 적용 이후에 attention 수행\n",
        "        self.attention_block = Residual(PreNorm(dim, WindowAttention(dim=dim,\n",
        "                                                                     heads=heads,\n",
        "                                                                     head_dim=head_dim,\n",
        "                                                                     shifted=shifted,\n",
        "                                                                     window_size=window_size,\n",
        "                                                                     relative_pos_embedding=relative_pos_embedding)))\n",
        "        # Residual : Residual Connection\n",
        "        # FeedForward : 일반적인 Transformer와 동일한 MLP 블록\n",
        "        self.mlp_block = Residual(PreNorm(dim, FeedForward(dim=dim, hidden_dim=mlp_dim)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.attention_block(x)\n",
        "        x = self.mlp_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eHW12_Lae_9s"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해상도는 줄이고 채널 수는 늘림 -> 정보 밀도 유지함\n",
        "\n",
        "class PatchMerging(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, downscaling_factor):\n",
        "        super().__init__()\n",
        "        self.downscaling_factor = downscaling_factor\n",
        "        self.patch_merge = nn.Unfold(kernel_size=downscaling_factor, stride=downscaling_factor, padding=0)\n",
        "\n",
        "        # 채널 수 증가 + feature transformation\n",
        "        self.linear = nn.Linear(in_channels * downscaling_factor ** 2, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape # 입력 : (batch, channels, height, width)\n",
        "        new_h, new_w = h // self.downscaling_factor, w // self.downscaling_factor # 해상도 계산\n",
        "        x = self.patch_merge(x).view(b, -1, new_h, new_w).permute(0, 2, 3, 1)\n",
        "        x = self.linear(x) # 각 패치 벡터를 새로운 차원으로 변환\n",
        "        return x"
      ],
      "metadata": {
        "id": "YFkDJIVifAjF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 구성\n",
        "\n",
        "class StageModule(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_dimension, layers, downscaling_factor, num_heads, head_dim, window_size,\n",
        "                 relative_pos_embedding):\n",
        "        super().__init__()\n",
        "        assert layers % 2 == 0, 'Stage layers need to be divisible by 2 for regular and shifted block.'\n",
        "\n",
        "        # PatchMerging : 입력 feature map을 다운스케일링 , 채널 수 증가시킴\n",
        "        self.patch_partition = PatchMerging(in_channels=in_channels, out_channels=hidden_dimension,\n",
        "                                            downscaling_factor=downscaling_factor)\n",
        "\n",
        "        # SwinBlock 쌍 구성\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(layers // 2):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=False, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "                SwinBlock(dim=hidden_dimension, heads=num_heads, head_dim=head_dim, mlp_dim=hidden_dimension * 4,\n",
        "                          shifted=True, window_size=window_size, relative_pos_embedding=relative_pos_embedding),\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_partition(x)\n",
        "        for regular_block, shifted_block in self.layers:\n",
        "            x = regular_block(x)\n",
        "            x = shifted_block(x)\n",
        "        return x.permute(0, 3, 1, 2)"
      ],
      "metadata": {
        "id": "FfgXzCFUk7UG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 계층적 Swin Transformer 구조\n",
        "\n",
        "class SwinTransformer(nn.Module):\n",
        "    def __init__(self, *, hidden_dim, layers, heads, img_size = 224, channels=3, num_classes=1000, head_dim=32, window_size=7,\n",
        "                 downscaling_factors=(4, 2, 2, 2), relative_pos_embedding=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # 총 4개의 stage\n",
        "        self.stage1 = StageModule(in_channels=channels, hidden_dimension=hidden_dim, layers=layers[0],\n",
        "                                  downscaling_factor=downscaling_factors[0], num_heads=heads[0], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage2 = StageModule(in_channels=hidden_dim, hidden_dimension=hidden_dim * 2, layers=layers[1],\n",
        "                                  downscaling_factor=downscaling_factors[1], num_heads=heads[1], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage3 = StageModule(in_channels=hidden_dim * 2, hidden_dimension=hidden_dim * 4, layers=layers[2],\n",
        "                                  downscaling_factor=downscaling_factors[2], num_heads=heads[2], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "        self.stage4 = StageModule(in_channels=hidden_dim * 4, hidden_dimension=hidden_dim * 8, layers=layers[3],\n",
        "                                  downscaling_factor=downscaling_factors[3], num_heads=heads[3], head_dim=head_dim,\n",
        "                                  window_size=window_size, relative_pos_embedding=relative_pos_embedding)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_dim * 8),\n",
        "            nn.Linear(hidden_dim * 8, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        x = self.stage1(img)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.stage4(x)\n",
        "        x = x.mean(dim=[2, 3]) # global 평균 -> classificatiob vector\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "# 사전 정의 모델\n",
        "\n",
        "# Swin -Tiny\n",
        "def swin_t(hidden_dim=96, layers=(2, 2, 6, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "# Swin -Small\n",
        "def swin_s(hidden_dim=96, layers=(2, 2, 18, 2), heads=(3, 6, 12, 24), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "# Swin -Base\n",
        "def swin_b(hidden_dim=128, layers=(2, 2, 18, 2), heads=(4, 8, 16, 32), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)\n",
        "\n",
        "# Swin -Large\n",
        "def swin_l(hidden_dim=192, layers=(2, 2, 18, 2), heads=(6, 12, 24, 48), **kwargs):\n",
        "    return SwinTransformer(hidden_dim=hidden_dim, layers=layers, heads=heads, **kwargs)"
      ],
      "metadata": {
        "id": "11VpD1ZcfHay"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tlbLfuJhqlNc",
        "outputId": "a9e4b27d-4161-4f2b-8653-5c1f38861baa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mProcessing /content/drive/MyDrive/Euron\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from swin-transformer-pytorch==0.4.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from swin-transformer-pytorch==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->swin-transformer-pytorch==0.4.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.0.2)\n",
            "Building wheels for collected packages: swin-transformer-pytorch\n",
            "  Building wheel for swin-transformer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swin-transformer-pytorch: filename=swin_transformer_pytorch-0.4.1-py3-none-any.whl size=6972 sha256=e1637a7534eac657adcb35650735883b9d27f25af9142724538222f1d831d952\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rry_7czu/wheels/25/02/bb/d1ed904da00fa1ee0558fa87298cfa417dc8ff9379e602a9f4\n",
            "Successfully built swin-transformer-pytorch\n",
            "Installing collected packages: swin-transformer-pytorch\n",
            "  Attempting uninstall: swin-transformer-pytorch\n",
            "    Found existing installation: swin-transformer-pytorch 0.4.1\n",
            "    Uninstalling swin-transformer-pytorch-0.4.1:\n",
            "      Successfully uninstalled swin-transformer-pytorch-0.4.1\n",
            "Successfully installed swin-transformer-pytorch-0.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "swin_transformer_pytorch"
                ]
              },
              "id": "b23dc5fa5720438b93a6beba619ea111"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup.py install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrH73mmLo4MU",
        "outputId": "3e088bc5-7c10-4a14-9235-dafbaf64b058"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer or other\n",
            "        standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing swin_transformer_pytorch.egg-info/PKG-INFO\n",
            "writing dependency_links to swin_transformer_pytorch.egg-info/dependency_links.txt\n",
            "writing requirements to swin_transformer_pytorch.egg-info/requires.txt\n",
            "writing top-level names to swin_transformer_pytorch.egg-info/top_level.txt\n",
            "reading manifest file 'swin_transformer_pytorch.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'swin_transformer_pytorch.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "warning: install_lib: 'build/lib' does not exist -- no Python modules to install\n",
            "\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying swin_transformer_pytorch.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying swin_transformer_pytorch.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying swin_transformer_pytorch.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying swin_transformer_pytorch.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying swin_transformer_pytorch.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/swin_transformer_pytorch-0.4.1-py3.11.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing swin_transformer_pytorch-0.4.1-py3.11.egg\n",
            "Copying swin_transformer_pytorch-0.4.1-py3.11.egg to /usr/local/lib/python3.11/dist-packages\n",
            "Adding swin-transformer-pytorch 0.4.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/swin_transformer_pytorch-0.4.1-py3.11.egg\n",
            "Processing dependencies for swin-transformer-pytorch==0.4.1\n",
            "Searching for nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-nvjitlink-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ff/ff/847841bacfbefc97a00036e0fce5a0f086b640756dc38caea5e1bb002655/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=06b3b9b25bf3f8af351d664978ca26a16d2c5127dbd53c0497e28d1fb9611d57\n",
            "Best match: nvidia-nvjitlink-cu12 12.4.127\n",
            "Processing nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-nvjitlink-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './swin_transformer_pytorch-0.4.1-py3.11.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cusparse-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/db/f7/97a9ea26ed4bbbfc2d470994b8b4f338ef663be97b8f677519ac195e113d/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl#sha256=ea4f11a2904e2a8dc4b1833cc1b5181cde564edd0d5cd33e3c168eff2d1863f1\n",
            "Best match: nvidia-cusparse-cu12 12.3.1.170\n",
            "Processing nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cusparse-cu12 12.3.1.170 to easy-install.pth file\n",
            "detected new path './nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cusolver-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/3a/e1/5b9089a4b2a4790dfdea8b3a006052cfecff58139d5a4e34cb1a51df8d6f/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl#sha256=19e33fa442bcfd085b3086c4ebf7e8debc07cfe01e11513cc6d332fd918ac260\n",
            "Best match: nvidia-cusolver-cu12 11.6.1.9\n",
            "Processing nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cusolver-cu12 11.6.1.9 to easy-install.pth file\n",
            "detected new path './nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-curand-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl#sha256=a88f583d4e0bb643c49743469964103aa59f7f708d862c3ddb0fc07f851e3b8b\n",
            "Best match: nvidia-curand-cu12 10.3.5.147\n",
            "Processing nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-curand-cu12 10.3.5.147 to easy-install.pth file\n",
            "detected new path './nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cufft-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/27/94/3266821f65b92b3138631e9c8e7fe1fb513804ac934485a8d05776e1dd43/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl#sha256=f083fc24912aa410be21fa16d157fed2055dab1cc4b6934a0e03cba69eb242b9\n",
            "Best match: nvidia-cufft-cu12 11.2.1.3\n",
            "Processing nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cufft-cu12 11.2.1.3 to easy-install.pth file\n",
            "detected new path './nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cublas-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ae/71/1c91302526c45ab494c23f61c7a84aa568b8c1f9d196efa5993957faf906/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl#sha256=2fc8da60df463fdefa81e323eef2e36489e1c94335b5358bcb38360adf75ac9b\n",
            "Best match: nvidia-cublas-cu12 12.4.5.8\n",
            "Processing nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cublas-cu12 12.4.5.8 to easy-install.pth file\n",
            "detected new path './nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cudnn-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl#sha256=165764f44ef8c61fcdfdfdbe769d687e06374059fbb388b6c89ecb0e28793a6f\n",
            "Best match: nvidia-cudnn-cu12 9.1.0.70\n",
            "Processing nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cudnn-cu12 9.1.0.70 to easy-install.pth file\n",
            "detected new path './nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-cupti-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/67/42/f4f60238e8194a3106d06a058d494b18e006c10bb2b915655bd9f6ea4cb1/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=9dec60f5ac126f7bb551c055072b69d85392b13311fcc1bcda2202d172df30fb\n",
            "Best match: nvidia-cuda-cupti-cu12 12.4.127\n",
            "Processing nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-cupti-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-runtime-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/ea/27/1795d86fe88ef397885f2e580ac37628ed058a92ed2c39dc8eac3adf0619/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=64403288fa2136ee8e467cdc9c9427e0434110899d07c779f25b5c068934faa5\n",
            "Best match: nvidia-cuda-runtime-cu12 12.4.127\n",
            "Processing nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-runtime-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\"\n",
            "Reading https://pypi.org/simple/nvidia-cuda-nvrtc-cu12/\n",
            "Downloading https://files.pythonhosted.org/packages/2c/14/91ae57cd4db3f9ef7aa99f4019cfa8d54cb4caa7e00975df6467e9725a9f/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl#sha256=a178759ebb095827bd30ef56598ec182b85547f1508941a3d560eb7ea1fbf338\n",
            "Best match: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "Processing nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl\n",
            "Installing nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl to /usr/local/lib/python3.11/dist-packages\n",
            "Adding nvidia-cuda-nvrtc-cu12 12.4.127 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Installed /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg\n",
            "Searching for einops==0.8.1\n",
            "Best match: einops 0.8.1\n",
            "Adding einops 0.8.1 to easy-install.pth file\n",
            "detected new path './nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg'\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for torch==2.6.0+cu124\n",
            "Best match: torch 2.6.0+cu124\n",
            "Adding torch 2.6.0+cu124 to easy-install.pth file\n",
            "Installing torchfrtrace script to /usr/local/bin\n",
            "Installing torchrun script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for sympy==1.13.1\n",
            "Best match: sympy 1.13.1\n",
            "Adding sympy 1.13.1 to easy-install.pth file\n",
            "Installing isympy script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for triton==3.2.0\n",
            "Best match: triton 3.2.0\n",
            "Adding triton 3.2.0 to easy-install.pth file\n",
            "Installing proton script to /usr/local/bin\n",
            "Installing proton-viewer script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-nvtx-cu12==12.4.127\n",
            "Best match: nvidia-nvtx-cu12 12.4.127\n",
            "Adding nvidia-nvtx-cu12 12.4.127 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-nccl-cu12==2.21.5\n",
            "Best match: nvidia-nccl-cu12 2.21.5\n",
            "Adding nvidia-nccl-cu12 2.21.5 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for nvidia-cusparselt-cu12==0.6.2\n",
            "Best match: nvidia-cusparselt-cu12 0.6.2\n",
            "Adding nvidia-cusparselt-cu12 0.6.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for fsspec==2025.3.2\n",
            "Best match: fsspec 2025.3.2\n",
            "Adding fsspec 2025.3.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for jinja2==3.1.6\n",
            "Best match: jinja2 3.1.6\n",
            "Adding jinja2 3.1.6 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for networkx==3.4.2\n",
            "Best match: networkx 3.4.2\n",
            "Adding networkx 3.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for typing-extensions==4.13.2\n",
            "Best match: typing-extensions 4.13.2\n",
            "Adding typing-extensions 4.13.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for filelock==3.18.0\n",
            "Best match: filelock 3.18.0\n",
            "Adding filelock 3.18.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for mpmath==1.3.0\n",
            "Best match: mpmath 1.3.0\n",
            "Adding mpmath 1.3.0 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Searching for MarkupSafe==3.0.2\n",
            "Best match: MarkupSafe 3.0.2\n",
            "Adding MarkupSafe 3.0.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.11/dist-packages\n",
            "Finished processing dependencies for swin-transformer-pytorch==0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7osys9Pn0EtW",
        "outputId": "abdd3c39-6405-489e-9175-1bd5035b7640"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/swin_transformer_pytorch-0.4.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
            "\u001b[0mProcessing /content/drive/MyDrive/Euron\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from swin-transformer-pytorch==0.4.1) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from swin-transformer-pytorch==0.4.1) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_nvrtc_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_runtime_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_cuda_cupti_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages/nvidia_cudnn_cu12-9.1.0.70-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages/nvidia_cublas_cu12-12.4.5.8-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages/nvidia_cufft_cu12-11.2.1.3-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages/nvidia_curand_cu12-10.3.5.147-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages/nvidia_cusolver_cu12-11.6.1.9-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages/nvidia_cusparse_cu12-12.3.1.170-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages/nvidia_nvjitlink_cu12-12.4.127-py3.11-linux-x86_64.egg (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.1->swin-transformer-pytorch==0.4.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.1->swin-transformer-pytorch==0.4.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.1->swin-transformer-pytorch==0.4.1) (3.0.2)\n",
            "Building wheels for collected packages: swin-transformer-pytorch\n",
            "  Building wheel for swin-transformer-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swin-transformer-pytorch: filename=swin_transformer_pytorch-0.4.1-py3-none-any.whl size=4321 sha256=8ad9148c5f58bc4f8523c4d627c5bfd8c52c709cdeac002b00013d78870df14d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-407y2kq2/wheels/25/02/bb/d1ed904da00fa1ee0558fa87298cfa417dc8ff9379e602a9f4\n",
            "Successfully built swin-transformer-pytorch\n",
            "Installing collected packages: swin-transformer-pytorch\n",
            "  Attempting uninstall: swin-transformer-pytorch\n",
            "    Found existing installation: swin-transformer-pytorch 0.4.1\n",
            "    Uninstalling swin-transformer-pytorch-0.4.1:\n",
            "      Successfully uninstalled swin-transformer-pytorch-0.4.1\n",
            "Successfully installed swin-transformer-pytorch-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# init.py\n",
        "from swin_transformer_pytorch.swin_transformer import SwinTransformer, swin_t, swin_s, swin_b, swin_l"
      ],
      "metadata": {
        "id": "Az0YwJiJb4gT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example.py\n",
        "\n",
        "import torch\n",
        "from swin_transformer_pytorch import SwinTransformer\n",
        "\n",
        "net = SwinTransformer(\n",
        "    hidden_dim=96,\n",
        "    layers=(2, 2, 6, 2),\n",
        "    heads=(3, 6, 12, 24),\n",
        "    channels=3,\n",
        "    num_classes=3,\n",
        "    head_dim=32,\n",
        "    window_size=7,\n",
        "    downscaling_factors=(4, 2, 2, 2),\n",
        "    relative_pos_embedding=True\n",
        ")\n",
        "dummy_x = torch.randn(1, 3, 224, 224)\n",
        "logits = net(dummy_x)  # (1,3)\n",
        "print(net)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-zNdUhz2B95",
        "outputId": "d22f5ef1-d124-43eb-c25a-ba86a794723d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SwinTransformer(\n",
            "  (stage1): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=4, dilation=1, padding=0, stride=4)\n",
            "      (linear): Linear(in_features=48, out_features=96, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
            "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=96, out_features=288, bias=False)\n",
            "                (to_out): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage2): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=384, out_features=192, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
            "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=192, out_features=576, bias=False)\n",
            "                (to_out): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage3): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=768, out_features=384, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0-2): 3 x ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
            "                (to_out): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (stage4): StageModule(\n",
            "    (patch_partition): PatchMerging(\n",
            "      (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
            "      (linear): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    )\n",
            "    (layers): ModuleList(\n",
            "      (0): ModuleList(\n",
            "        (0): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1): SwinBlock(\n",
            "          (attention_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): WindowAttention(\n",
            "                (cyclic_shift): CyclicShift()\n",
            "                (cyclic_back_shift): CyclicShift()\n",
            "                (to_qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
            "                (to_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (mlp_block): Residual(\n",
            "            (fn): PreNorm(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (fn): FeedForward(\n",
            "                (net): Sequential(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): GELU(approximate='none')\n",
            "                  (2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (mlp_head): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Linear(in_features=768, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "tensor([[ 0.1316,  1.0847, -1.0691]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실제 데이터 로드해보기(CIFAR10)"
      ],
      "metadata": {
        "id": "cZ-220D12lU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = Compose([Resize((224, 224)), ToTensor()])\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwyhxBhx2k4u",
        "outputId": "51484fa5-f909-4036-aecb-dac695f062ef"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:04<00:00, 41.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# swin transformer 불러오기\n",
        "from swin_transformer_pytorch.swin_transformer import SwinTransformer\n",
        "import torch.nn as nn\n",
        "\n",
        "model = SwinTransformer(img_size=224, patch_size=4, in_chans=3, num_classes=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "Phxygb9d25Xo",
        "outputId": "428cf7a6-ee13-4f7b-9ecd-8465c2e5dbd8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "SwinTransformer.__init__() got an unexpected keyword argument 'img_size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d6c01434511c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSwinTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: SwinTransformer.__init__() got an unexpected keyword argument 'img_size'"
          ]
        }
      ]
    }
  ]
}